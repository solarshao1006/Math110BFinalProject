{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpS7hI70Ho1l"
   },
   "source": [
    "# Compression with Constraints: Steganography\n",
    "\n",
    "The natural images are compressible, usually a full size image of several megabytes can be reduced to kilobytes level easily without lossing too much important information. Such property has been used widely to denoising, deblur, etc. techniques. \n",
    "\n",
    "The [``steganography``](https://en.wikipedia.org/wiki/Steganography) is a topic lying in the cryptography. It is concealing a file, message, image, or video within another file, message, image, or video. \n",
    "\n",
    "The advantage of steganography over cryptography alone is that the intended secret message ***does not attract attention to itself as an object of scrutiny***. Plainly visible encrypted messages, no matter how unbreakable they are, arouse interest and may in themselves be incriminating in countries in which encryption is illegal. \n",
    "\n",
    "Whereas cryptography is the practice of protecting the contents of a message alone, steganography is concerned both with concealing the fact that a secret message is being sent and its contents.\n",
    "\n",
    "Steganography includes the concealment of information within computer files. In digital steganography, electronic communications may include steganographic coding inside of a transport layer, such as a document file, image file, program or protocol. Media files are ideal for steganographic transmission because of their large size. For example, a sender might start with an innocuous image file and adjust the color of every hundredth pixel to correspond to a letter in the alphabet. The change is so subtle that someone who is not specifically looking for it is unlikely to notice the change. \n",
    "\n",
    "In this project, we deal with a special case: stegranography with images only. So unlike many practical scenes, for instance, encrypt text, document in images, this task might not be able to produce perfect recovery of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocl_uhidJN92"
   },
   "source": [
    "## Purpose of the project\n",
    "\n",
    "The project is not meant to create super powerful technique to conceal information, it is more concerned to get used to imaging processing libraries and optimization techniques. On the other hand, it also provides a challenge to think about how to detect the ``steganographic`` images without the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gQlN3z3KrMC"
   },
   "source": [
    "## Mathematical aspects\n",
    "\n",
    "The stegranography has two important components: encryption and decryption. \n",
    "\n",
    "Suppose you have an original image and a secret image.\n",
    "\n",
    "\n",
    "1.   Encryption: As a sender, your task is to make the original image and secret image merge into one image. The purpose is two fold. Firstly, your outcome must be an image, if this image stays far away from the original image, then it will attract other people's attention. That will count as a failure. Secondly, your outcome image must also convey the information of the secret image. It is not that simple, since any blending of information will change each other. The problem is how much we can afford.\n",
    "2.   Decryption: After the encryption part, your outcome image will have two parts of information coming from original image and the secret image. As the receiver, your task will be inverting the encryption process, to recover the secret image as much as possible (Caution,  the receiver do not care about original image). \n",
    "\n",
    "If we mathematically represent such process, let $x$ be the original image, $y$ is the secret image, then $z = E(x, y)$ is the encrypted image, $E$ is the encryption function. You will try to minimize \n",
    "$$\\|z - x\\|$$\n",
    "The above norm is in certain sense, we will discuss that later. However, above minimization will subject to another constraint, which is the decryption function $D$ can recover sufficient information of the secret image. That is \n",
    "$$\\|D(z) - y\\|$$\n",
    "should be as small as possible.\n",
    "\n",
    "It is possible to construct a unified objective function:\n",
    "$$\\min_{E, D} \\|x - E(x,y)\\| + \\gamma \\|y - D(z)\\|$$\n",
    "where $E$ and $D$ are the parameters to find. $\\gamma$ is a parameter chosen at your choice.\n",
    "\n",
    "Of course, there are other constraints from the images, because images are pixels, each pixel contains 3 channels: R,G,B, each one is a 8-bit integer, goes from 0 to 255. If the image has 4 channels RGBA, then it will provide more information. Therefore above optimization problem also has constraints that $E(x,y)$ and $D(z)$ must be images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OSAQ_vKO58g"
   },
   "source": [
    "## Algorithm 101, LSB\n",
    "\n",
    "The LSB is called least significant bits, which means you will replace the least significant bits of the original image with the secret image's most significant bits. This method will kill some information from both images, but the performance seems OK for general cases.\n",
    "\n",
    "Here are a few references on this simple algorithm: \n",
    "\n",
    "0.   https://towardsdatascience.com/steganography-hiding-an-image-inside-another-77ca66b2acb1, the code is [here](https://github.com/kelvins/steganography)\n",
    "1.   https://github.com/RobinDavid/LSB-Steganography \n",
    "2.   https://pdfs.semanticscholar.org/3dce/b6307cee042b687b7f377ec1d5de91ce20b0.pdf\n",
    "3.   https://hackernoon.com/simple-image-steganography-in-python-18c7b534854f\n",
    "\n",
    "The basic idea is (suppose you have a code to turn int8 into binary string),  inside each channel, say R, your original image's pixel, say represented as ``1001,0011``, and your secret image's that pixel is ``1110,1101``, then replace the last 4 bits in original image's pixel with the first 4 bits of secret image's corresponding pixel, the resulting number will be ``1001,1110``. In this way, the change in the original image could be small (on average).  There are other ways to alter the LSB, like treating the secret image as a binary string , and evenly distribute to each pixel. ***In our case, for simplicity, we only consider the images with the same size.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrdLQZrjWYOX"
   },
   "source": [
    "## Shortcomings \n",
    "\n",
    "The shortcomings of algorithms/methods are mainly on the detection, which is, we can easily detect your outcome image is not feeling right. \n",
    "\n",
    "In practice, if you look at the altered image, say from LSB, you do not feel anything. But the LSB has a very obvious drawback: it alters the last bit, which may distory the statistics of the last bit. In theory, the last bit 0 and 1 should obey certain heuristic distribution in the image, but now it will be changed.\n",
    "\n",
    "The detection code is here: https://github.com/b3dk7/StegExpose\n",
    "\n",
    "In https://dl.acm.org/citation.cfm?id=1929317, the paper introduced the method to preserve the statistics. \n",
    "\n",
    "In https://pdfs.semanticscholar.org/80a5/fcbeda7697d9641bc80460593c2f8f305a65.pdf, it introduced the detection of LSB. \n",
    "\n",
    "\n",
    "In http://futuremedia.szu.edu.cn/assets/files/CF_What%20makes%20the%20stego%20imageundetectable.pdf, the authors considered choosing the best original image to hide the given secret image \n",
    "\n",
    "Again,  currently, we are not supposed to consider this far, but it will be a future work if you find this interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4frfwD5WBRx"
   },
   "source": [
    "## Other ways\n",
    "\n",
    "The review paper (maybe old) is found here: https://www.sciencedirect.com/science/article/pii/B9780123855107000023\n",
    "\n",
    "(some other reviews are found: [here](https://pdfs.semanticscholar.org/57a1/d15dcbf946f093a59db55f8828699fef7826.pdf) and [here](https://www.cscjournals.org/manuscript/Journals/IJCSS/Volume6/Issue3/IJCSS-670.pdf))\n",
    "\n",
    "\n",
    "1.   https://arxiv.org/pdf/1606.05294.pdf. In this paper, it introduces the method to use NN to replace (learn) the LSB process. \n",
    "2.   https://papers.nips.cc/paper/6802-hiding-images-in-plain-sight-deep-steganography.pdf, it introduced a NN to  approximate $D$ and $E$. \n",
    "3. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.7157&rep=rep1&type=pdf, it uses the DCT (discrete-cosine-transform) and LSB. \n",
    "4. https://arxiv.org/pdf/1806.06357.pdf and  [code](https://github.com/adamcavendish/Deep-Image-Steganography)\n",
    "5. https://ieeexplore.ieee.org/document/8403208/all-figures\n",
    "6. https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jiren_Zhu_HiDDeN_Hiding_Data_ECCV_2018_paper.pdf\n",
    "7. https://arxiv.org/pdf/1904.01444.pdf\n",
    "8. https://link.springer.com/article/10.1007/s00521-014-1702-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kzRMcHxchJ0"
   },
   "source": [
    "## First task\n",
    "Implement LSB, the images (orignal and secret) are of the same sizes. If you do not want to implement any, at least go through the code  [here](https://github.com/kelvins/steganography). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2tUGx1UKpVO",
    "outputId": "6675932d-aefe-4593-92d8-bd20f6b1c3b3"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yiacaQMAKsjL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kJCQNUKgKvQj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy2OdJhbLLpC",
    "outputId": "1ad90f69-d94e-45cb-97ef-742a94fed803",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'charmap' codec can't encode characters in position 69-70: character maps to <undefined>\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download gaz3ll3/optimization-ii-project-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlNiYBRsLfYa",
    "outputId": "947fde30-2e8b-4d67-cb07-1ade5064c1d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! unzip 'optimization-ii-project-3.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ukk3cb7M-7e",
    "outputId": "906b6175-cf2c-43ec-fe42-c2a3df89d1d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'archive',\n",
       " 'archive.zip',\n",
       " 'kaggle',\n",
       " 'kaggle.json',\n",
       " '“Steg_ipynb”的副本.ipynb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orEqpHwxaywC"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from io import BytesIO\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "BvhZ8Qn0JT1v",
    "outputId": "e8fd67a1-03f5-4d17-8cb1-d2941b760330"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2aae1d46-fe02-404f-ab6b-30da4ba23d1e\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2aae1d46-fe02-404f-ab6b-30da4ba23d1e\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Upload images\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ktta1KWOiom6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#Check current directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8wabGBHZyAt"
   },
   "outputs": [],
   "source": [
    "import click\n",
    "import PIL\n",
    "\n",
    "class Steganography:\n",
    "\n",
    "    @staticmethod\n",
    "    def __int_to_bin(rgb):\n",
    "        \"\"\"Convert an integer tuple to a binary (string) tuple.\n",
    "        :param rgb: An integer tuple (e.g. (220, 110, 96))\n",
    "        :return: A string tuple (e.g. (\"00101010\", \"11101011\", \"00010110\"))\n",
    "        \"\"\"\n",
    "        r, g, b = rgb\n",
    "        return (f'{r:08b}',\n",
    "                f'{g:08b}',\n",
    "                f'{b:08b}')\n",
    "\n",
    "    @staticmethod\n",
    "    def __bin_to_int(rgb):\n",
    "        \"\"\"Convert a binary (string) tuple to an integer tuple.\n",
    "        :param rgb: A string tuple (e.g. (\"00101010\", \"11101011\", \"00010110\"))\n",
    "        :return: Return an int tuple (e.g. (220, 110, 96))\n",
    "        \"\"\"\n",
    "        r, g, b = rgb\n",
    "        return (int(r, 2),\n",
    "                int(g, 2),\n",
    "                int(b, 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def __merge_rgb(rgb1, rgb2):\n",
    "        \"\"\"Merge two RGB tuples.\n",
    "        :param rgb1: A string tuple (e.g. (\"00101010\", \"11101011\", \"00010110\"))\n",
    "        :param rgb2: Another string tuple\n",
    "        (e.g. (\"00101010\", \"11101011\", \"00010110\"))\n",
    "        :return: An integer tuple with the two RGB values merged.\n",
    "        \"\"\"\n",
    "        r1, g1, b1 = rgb1\n",
    "        r2, g2, b2 = rgb2\n",
    "        rgb = (r1[:4] + r2[:4],\n",
    "               g1[:4] + g2[:4],\n",
    "               b1[:4] + b2[:4])\n",
    "        return rgb\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(img1, img2):\n",
    "        \"\"\"Merge two images. The second one will be merged into the first one.\n",
    "        :param img1: First image\n",
    "        :param img2: Second image\n",
    "        :return: A new merged image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check the images dimensions\n",
    "        if img2.size[0] > img1.size[0] or img2.size[1] > img1.size[1]:\n",
    "            raise ValueError('Image 2 should not be larger than Image 1!')\n",
    "\n",
    "        # Get the pixel map of the two images\n",
    "        pixel_map1 = img1.load()\n",
    "        pixel_map2 = img2.load()\n",
    "\n",
    "        # Create a new image that will be outputted\n",
    "        new_image = PIL.Image.new(img1.mode, img1.size)\n",
    "        pixels_new = new_image.load()\n",
    "\n",
    "        for i in range(img1.size[0]):\n",
    "            for j in range(img1.size[1]):\n",
    "                rgb1 = Steganography.__int_to_bin(pixel_map1[i, j])\n",
    "\n",
    "                # Use a black pixel as default\n",
    "                rgb2 = Steganography.__int_to_bin((0, 0, 0))\n",
    "\n",
    "                # Check if the pixel map position is valid for the second image\n",
    "                if i < img2.size[0] and j < img2.size[1]:\n",
    "                    rgb2 = Steganography.__int_to_bin(pixel_map2[i, j])\n",
    "\n",
    "                # Merge the two pixels and convert it to a integer tuple\n",
    "                rgb = Steganography.__merge_rgb(rgb1, rgb2)\n",
    "\n",
    "                pixels_new[i, j] = Steganography.__bin_to_int(rgb)\n",
    "\n",
    "        return new_image\n",
    "\n",
    "    @staticmethod\n",
    "    def unmerge(img):\n",
    "        \"\"\"Unmerge an image.\n",
    "        :param img: The input image.\n",
    "        :return: The unmerged/extracted image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the pixel map\n",
    "        pixel_map = img.load()\n",
    "\n",
    "        # Create the new image and load the pixel map\n",
    "        new_image = PIL.Image.new(img.mode, img.size)\n",
    "        pixels_new = new_image.load()\n",
    "\n",
    "        # Tuple used to store the image original size\n",
    "        original_size = img.size\n",
    "\n",
    "        for i in range(img.size[0]):\n",
    "            for j in range(img.size[1]):\n",
    "                # Get the RGB (as a string tuple) from the current pixel\n",
    "                r, g, b = Steganography.__int_to_bin(pixel_map[i, j])\n",
    "\n",
    "                # Extract the last 4 bits (corresponding to the hidden image)\n",
    "                # Concatenate 4 zero bits because we are working with 8 bit\n",
    "                rgb = (r[4:] + '0000',\n",
    "                       g[4:] + '0000',\n",
    "                       b[4:] + '0000')\n",
    "\n",
    "                # Convert it to an integer tuple\n",
    "                pixels_new[i, j] = Steganography.__bin_to_int(rgb)\n",
    "\n",
    "                # If this is a 'valid' position, store it\n",
    "                # as the last valid position\n",
    "                if pixels_new[i, j] != (0, 0, 0):\n",
    "                    original_size = (i + 1, j + 1)\n",
    "\n",
    "        # Crop the image based on the 'valid' pixels\n",
    "        new_image = new_image.crop((0, 0, original_size[0], original_size[1]))\n",
    "\n",
    "        return new_image\n",
    "\n",
    "def merge(img1, img2, output):\n",
    "    merged_image = Steganography.merge(PIL.Image.open(img1), PIL.Image.open(img2))\n",
    "    merged_image.save(output)\n",
    "\n",
    "def unmerge(img, output):\n",
    "    unmerged_image = Steganography.unmerge(PIL.Image.open(img))\n",
    "    unmerged_image.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln8EQ8T4f5uV"
   },
   "outputs": [],
   "source": [
    "#Call the function directly instead of using the command line\n",
    "merge('img1.jpg', 'img2.jpg', 'output_merge.png')\n",
    "unmerge('output_merge.png', 'output_unmerge.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y61a05RcV-e_"
   },
   "outputs": [],
   "source": [
    "merge('mountain/art1131.jpg', 'mountain/land10.jpg', 'output2_merge.png')\n",
    "display(Image('output2_merge.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9rrMsXziiFR"
   },
   "outputs": [],
   "source": [
    "#Check the current directory list now, the outputs are in the directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vu-1ptuniu7L"
   },
   "outputs": [],
   "source": [
    "#Visualization\n",
    "display(Image('output_merge.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhfzmqDpmkeV"
   },
   "outputs": [],
   "source": [
    "#Visualization\n",
    "display(Image('output_unmerge.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJXJ-GRif-aN"
   },
   "source": [
    "## Second task\n",
    "Try to use neural network to approximate $D$ and $E$, the parameters are up to you, the structure is up to you.  This paper provides a good insight: https://papers.nips.cc/paper/6802-hiding-images-in-plain-sight-deep-steganography.pdf,  an implementation is found here: https://github.com/fpingham/DeepSteg/blob/master/DeepSteganography.ipynb, https://github.com/Ankit-Dhankhar/deep-steg/blob/master/steg%20net.py, and https://github.com/mr3coi/deepsteg and https://github.com/alexandremuzio/deep-steg and https://github.com/harveyslash/Deep-Steganography....., a blog https://buzzrobot.com/hiding-images-using-ai-deep-steganography-b7726bd58b06\n",
    "\n",
    "For the network structure, you can borrow the idea from autoencoder for the $E$ part, in that paper, the authors claimed the $E$ part uses 5 layers of convolutional neural networks with 3x3, 4x4, 5x5 patches. The idea is only to approximate the mappings $D$ and $E$, the fully connected network should also work, but convolutional type is cheaper. \n",
    "\n",
    "A good way to combine DCT (discrete-cosine-transform) to reduce the information first on secret images (bypassing the prep network in the paper). References are easy to find by searching google with DCT keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QvlOxlmWYS4W"
   },
   "outputs": [],
   "source": [
    "# Imports necessary libraries and modules\n",
    "from itertools import islice\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import utils\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os \n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import datasets, utils\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from random import shuffle\n",
    "from IPython.display import Image\n",
    "#from google.colab import drive\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "av41IhjFR3CP"
   },
   "outputs": [],
   "source": [
    "# load all 256*256 pictures\n",
    "# randomly split 80% as training data and 20% as validating data\n",
    "# for each set of data, you take first half as cover and second half as secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eKZMjhu0Soyx"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  '''\n",
    "  Get the directory path name of all pictures\n",
    "  '''\n",
    "  data_list = []\n",
    "  #dir_list = [x[0] for x in os.walk('data')]\n",
    "  for folder_dir in ['highway']:\n",
    "    file_dirlist = os.listdir(folder_dir)\n",
    "    for file_dir in file_dirlist:\n",
    "      split = os.path.splitext(file_dir)\n",
    "      if split[1] == '.jpg' or split[1] == '.png' or split[1] == '.jpeg':\n",
    "        file_full_path = os.path.join(folder_dir, file_dir)\n",
    "        image_tensor = torchvision.io.read_image(file_full_path) \n",
    "        data_list.append(image_tensor.div(256))\n",
    "  return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mImOgIxZpN3B"
   },
   "outputs": [],
   "source": [
    "def create_train_test_set():\n",
    "  '''\n",
    "  Create Train Set and Test Set; 80% of the data are randomly chosen as train set and the rest are test set.\n",
    "  '''\n",
    "  file_dir = load_data()\n",
    "  random.shuffle(file_dir)\n",
    "  train_set = file_dir[:int(0.8*len(file_dir))]\n",
    "  test_set = file_dir[int(0.8*len(file_dir)):]\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-MgXsTt6fmnc"
   },
   "outputs": [],
   "source": [
    "def separate_cover_secret(train_set, test_set):\n",
    "  train_set_cover = train_set[int(len(train_set) / 2):]\n",
    "  train_set_secret = train_set[:int(len(train_set) / 2)]\n",
    "\n",
    "  test_set_cover = test_set[int(len(test_set) / 2):]\n",
    "  test_set_secret = test_set[:int(len(test_set) / 2)]\n",
    "  return train_set_cover, train_set_secret, test_set_cover, test_set_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m3lIQ2yeRb5E"
   },
   "outputs": [],
   "source": [
    "#Create Training Set and Test set, cover_set and secret set\n",
    "train_set, test_set = create_train_test_set()\n",
    "train_set_cover, train_set_secret, test_set_cover, test_set_secret = separate_cover_secret(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C8z-Z7-3izoq"
   },
   "outputs": [],
   "source": [
    "class PrepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrepNetwork, self).__init__()\n",
    "        self.initialP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,50,kernel_size = 3, padding=1)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(50,50,kernel_size = 3, padding=1)),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('conv3', nn.Conv2d(50,50, kernel_size=3, padding=1)),\n",
    "          ('relu3', nn.ReLU()),\n",
    "          ('conv4', nn.Conv2d(50,50, kernel_size=3, padding=1)),\n",
    "          ('relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.initialP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv5', nn.Conv2d(1,50,kernel_size = 4, padding=1)),\n",
    "          ('relu5', nn.ReLU()),\n",
    "          ('conv6', nn.Conv2d(50,50,kernel_size = 4, padding=2)),\n",
    "          ('relu6', nn.ReLU()),\n",
    "          ('conv7', nn.Conv2d(50,50, kernel_size=4, padding=1)),\n",
    "          ('relu7', nn.ReLU()),\n",
    "          ('conv8', nn.Conv2d(50,50, kernel_size=4, padding=2)),\n",
    "          ('relu8', nn.ReLU())\n",
    "        ]))\n",
    "        self.initialP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv9', nn.Conv2d(1,50,kernel_size = 5, padding=2)),\n",
    "          ('relu9', nn.ReLU()),\n",
    "          ('conv10', nn.Conv2d(50,50,kernel_size = 5, padding=2)),\n",
    "          ('relu10', nn.ReLU()),\n",
    "          ('conv11', nn.Conv2d(50,50, kernel_size= 5, padding=2)),\n",
    "          ('relu11', nn.ReLU()),\n",
    "          ('conv12', nn.Conv2d(50,50, kernel_size=5, padding=2)),\n",
    "          ('relu12', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv13', nn.Conv2d(150,50,kernel_size = 3, padding=1)),\n",
    "          ('relu13', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv14', nn.Conv2d(150,50,kernel_size = 4, padding=1)),\n",
    "          ('relu14', nn.ReLU()),\n",
    "          ('conv15', nn.Conv2d(50, 50, kernel_size=4, padding=2)),\n",
    "          ('relu15', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv16', nn.Conv2d(150,50, kernel_size = 5, padding=2)),\n",
    "          ('relu16', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "\n",
    "    def forward(self, p):\n",
    "        p1 = self.initialP3(p)\n",
    "        p2 = self.initialP4(p)\n",
    "        p3 = self.initialP5(p)\n",
    "        mid = torch.cat((p1, p2, p3), 1)\n",
    "        p4 = self.finalP3(mid)\n",
    "        p5 = self.finalP4(mid)\n",
    "        p6 = self.finalP5(mid)\n",
    "        out = torch.cat((p4, p5, p6), 1)\n",
    "        return out\n",
    "\n",
    "# Hiding Network (5 conv layers)\n",
    "class HidingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HidingNetwork, self).__init__()\n",
    "        self.initialH3 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialH4 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialH5 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalH4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, h, device):\n",
    "        h1 = self.initialH3(h)\n",
    "        h2 = self.initialH4(h)\n",
    "        h3 = self.initialH5(h)\n",
    "        mid = torch.cat((h1, h2, h3), 1)\n",
    "        h4 = self.finalH3(mid)\n",
    "        h5 = self.finalH4(mid)\n",
    "        h6 = self.finalH5(mid)\n",
    "        mid2 = torch.cat((h4, h5, h6), 1)\n",
    "        out = self.finalH(mid2)\n",
    "        noise = torch.nn.init.normal(torch.Tensor(out.data.size()), 0, 0.1).to(device)\n",
    "        out_noise =  Variable(out.data + noise)\n",
    "        return out, out_noise.to(device)\n",
    "\n",
    "# Reveal Network (2 conv layers)\n",
    "class RevealNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevealNetwork, self).__init__()\n",
    "        self.initialR3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialR4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialR5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalR4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, r):\n",
    "        r1 = self.initialR3(r)\n",
    "        r2 = self.initialR4(r)\n",
    "        r3 = self.initialR5(r)\n",
    "        mid = torch.cat((r1, r2, r3), 1)\n",
    "        r4 = self.finalR3(mid)\n",
    "        r5 = self.finalR4(mid)\n",
    "        r6 = self.finalR5(mid)\n",
    "        mid2 = torch.cat((r4, r5, r6), 1)\n",
    "        out = self.finalR(mid2)\n",
    "        return out\n",
    "\n",
    "# Join three networks in one module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.m1 = PrepNetwork()\n",
    "        self.m2 = HidingNetwork()\n",
    "        self.m3 = RevealNetwork()\n",
    "\n",
    "    def forward(self, secret, cover, device):\n",
    "        x_1 = self.m1(secret)\n",
    "        mid = torch.cat((x_1, cover), 1)\n",
    "        x_2, x_2_noise = self.m2(mid, device)\n",
    "        x_3 = self.m3(x_2_noise)\n",
    "        return x_2, x_3\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FB-LfafmZrkb"
   },
   "outputs": [],
   "source": [
    "# Creates net object\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ydyixovD3PYK"
   },
   "outputs": [],
   "source": [
    "def customized_loss(S_prime, C_prime, S, C, B):\n",
    "    ''' Calculates loss specified on the paper.'''\n",
    "    \n",
    "    loss_cover = torch.sqrt(torch.nn.functional.mse_loss(C_prime, C))\n",
    "    loss_secret = torch.sqrt(torch.nn.functional.mse_loss(S_prime, S))\n",
    "    loss_all = loss_cover + B * loss_secret\n",
    "    return loss_all, loss_cover, loss_secret\n",
    "\n",
    "def denormalize(image, std, mean):\n",
    "    ''' Denormalizes a tensor of images.'''\n",
    "\n",
    "    for t in range(3):\n",
    "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
    "    return image\n",
    "\n",
    "def imshow(img, idx, learning_rate, beta):\n",
    "    '''Prints out an image given in tensor format.'''\n",
    "    \n",
    "    img = denormalize(img, std, mean)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#def gaussian(tensor, device, mean=0, stddev=0.1):\n",
    "#    '''Adds random noise to a tensor.'''\n",
    "#    \n",
    "#    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n",
    "#    return Variable(tensor + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, train_secrets, train_covers, beta, learning_rate, num_epochs):\n",
    "    \n",
    "    # Save optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    # Iterate over batches performing forward and backward passes\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "        net.to('cuda')\n",
    "        train_losses = []\n",
    "        # Train one epoch\n",
    "        #for idx, res_lst in enumerate(train_set):\n",
    "        for idx in range(len(train_set_cover)):\n",
    "            train_covers = train_set_cover[idx].float().unsqueeze(1)\n",
    "            train_secrets = train_set_secret[idx].float().unsqueeze(1)\n",
    "\n",
    "            # Creates variable from secret and cover images\n",
    "            train_secrets = Variable(train_secrets).to('cuda')\n",
    "            train_covers = Variable(train_covers).to('cuda')\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_hidden, train_output = net(train_secrets, train_covers, 'cuda')\n",
    "\n",
    "            # Calculate loss and perform backprop\n",
    "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Saves training loss\n",
    "  \n",
    "            train_losses.append(train_loss.data.to('cpu'))\n",
    "            loss_history.append(train_loss.data.to('cpu'))\n",
    "            \n",
    "            # Prints mini-batch losses\n",
    "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, int(len(train_set)/2), train_loss.data, train_loss_cover.data, train_loss_secret.data))\n",
    "    \n",
    "        #torch.save(net.state_dict(), MODELS_PATH+'Epoch N{}.pkl'.format(epoch+1))\n",
    "        #print(train_losses)\n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "    \n",
    "        # Prints epoch average loss\n",
    "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
    "                epoch+1, num_epochs, mean_train_loss))\n",
    "    \n",
    "    return net, mean_train_loss, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "58YCc5krm0c4",
    "outputId": "893f19c5-fc2b-498c-d7a4-46a71438db63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-40fe894c6fe5>:116: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = torch.nn.init.normal(torch.Tensor(out.data.size()), 0, 0.1).to(device)\n",
      "<ipython-input-10-95a7298b6f53>:4: UserWarning: Using a target size (torch.Size([3, 1, 256, 256])) that is different to the input size (torch.Size([3, 3, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_cover = torch.sqrt(torch.nn.functional.mse_loss(C_prime, C))\n",
      "<ipython-input-10-95a7298b6f53>:5: UserWarning: Using a target size (torch.Size([3, 1, 256, 256])) that is different to the input size (torch.Size([3, 3, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_secret = torch.sqrt(torch.nn.functional.mse_loss(S_prime, S))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 1/104. Loss of 1.1594, cover loss of 0.5799, secret loss of 0.5795\n",
      "Training: Batch 2/104. Loss of 1.0805, cover loss of 0.6374, secret loss of 0.4431\n",
      "Training: Batch 3/104. Loss of 1.0397, cover loss of 0.5728, secret loss of 0.4669\n",
      "Training: Batch 4/104. Loss of 0.8654, cover loss of 0.5318, secret loss of 0.3336\n",
      "Training: Batch 5/104. Loss of 1.0530, cover loss of 0.5266, secret loss of 0.5264\n",
      "Training: Batch 6/104. Loss of 1.1185, cover loss of 0.5231, secret loss of 0.5954\n",
      "Training: Batch 7/104. Loss of 1.0820, cover loss of 0.5330, secret loss of 0.5490\n",
      "Training: Batch 8/104. Loss of 1.1607, cover loss of 0.5415, secret loss of 0.6193\n",
      "Training: Batch 9/104. Loss of 1.1757, cover loss of 0.5698, secret loss of 0.6059\n",
      "Training: Batch 10/104. Loss of 1.0492, cover loss of 0.5436, secret loss of 0.5056\n",
      "Training: Batch 11/104. Loss of 1.0496, cover loss of 0.5294, secret loss of 0.5202\n",
      "Training: Batch 12/104. Loss of 0.8957, cover loss of 0.4710, secret loss of 0.4246\n",
      "Training: Batch 13/104. Loss of 0.9576, cover loss of 0.4346, secret loss of 0.5230\n",
      "Training: Batch 14/104. Loss of 0.8415, cover loss of 0.4146, secret loss of 0.4269\n",
      "Training: Batch 15/104. Loss of 0.7592, cover loss of 0.3242, secret loss of 0.4350\n",
      "Training: Batch 16/104. Loss of 0.6515, cover loss of 0.3570, secret loss of 0.2946\n",
      "Training: Batch 17/104. Loss of 0.5170, cover loss of 0.3028, secret loss of 0.2142\n",
      "Training: Batch 18/104. Loss of 0.6941, cover loss of 0.4446, secret loss of 0.2496\n",
      "Training: Batch 19/104. Loss of 0.4661, cover loss of 0.2667, secret loss of 0.1994\n",
      "Training: Batch 20/104. Loss of 0.4052, cover loss of 0.2171, secret loss of 0.1880\n",
      "Training: Batch 21/104. Loss of 0.4855, cover loss of 0.2219, secret loss of 0.2636\n",
      "Training: Batch 22/104. Loss of 0.6691, cover loss of 0.4534, secret loss of 0.2156\n",
      "Training: Batch 23/104. Loss of 0.5079, cover loss of 0.2681, secret loss of 0.2399\n",
      "Training: Batch 24/104. Loss of 0.5929, cover loss of 0.3010, secret loss of 0.2919\n",
      "Training: Batch 25/104. Loss of 0.6127, cover loss of 0.4037, secret loss of 0.2090\n",
      "Training: Batch 26/104. Loss of 0.5055, cover loss of 0.3055, secret loss of 0.2000\n",
      "Training: Batch 27/104. Loss of 0.3279, cover loss of 0.1815, secret loss of 0.1464\n",
      "Training: Batch 28/104. Loss of 0.4918, cover loss of 0.2707, secret loss of 0.2211\n",
      "Training: Batch 29/104. Loss of 0.6294, cover loss of 0.3664, secret loss of 0.2630\n",
      "Training: Batch 30/104. Loss of 0.4334, cover loss of 0.2022, secret loss of 0.2312\n",
      "Training: Batch 31/104. Loss of 0.4597, cover loss of 0.2208, secret loss of 0.2389\n",
      "Training: Batch 32/104. Loss of 0.3790, cover loss of 0.1435, secret loss of 0.2355\n",
      "Training: Batch 33/104. Loss of 0.3748, cover loss of 0.1276, secret loss of 0.2473\n",
      "Training: Batch 34/104. Loss of 0.3673, cover loss of 0.1875, secret loss of 0.1798\n",
      "Training: Batch 35/104. Loss of 0.5813, cover loss of 0.2190, secret loss of 0.3623\n",
      "Training: Batch 36/104. Loss of 0.5013, cover loss of 0.2962, secret loss of 0.2052\n",
      "Training: Batch 37/104. Loss of 0.5266, cover loss of 0.3115, secret loss of 0.2151\n",
      "Training: Batch 38/104. Loss of 0.4396, cover loss of 0.2129, secret loss of 0.2268\n",
      "Training: Batch 39/104. Loss of 0.3369, cover loss of 0.1443, secret loss of 0.1926\n",
      "Training: Batch 40/104. Loss of 0.4346, cover loss of 0.2824, secret loss of 0.1522\n",
      "Training: Batch 41/104. Loss of 0.3045, cover loss of 0.1854, secret loss of 0.1192\n",
      "Training: Batch 42/104. Loss of 0.3841, cover loss of 0.1957, secret loss of 0.1884\n",
      "Training: Batch 43/104. Loss of 0.4269, cover loss of 0.1974, secret loss of 0.2295\n",
      "Training: Batch 44/104. Loss of 0.3100, cover loss of 0.1398, secret loss of 0.1702\n",
      "Training: Batch 45/104. Loss of 0.3480, cover loss of 0.1317, secret loss of 0.2163\n",
      "Training: Batch 46/104. Loss of 0.3778, cover loss of 0.2464, secret loss of 0.1314\n",
      "Training: Batch 47/104. Loss of 0.2655, cover loss of 0.1229, secret loss of 0.1426\n",
      "Training: Batch 48/104. Loss of 0.3867, cover loss of 0.2008, secret loss of 0.1859\n",
      "Training: Batch 49/104. Loss of 0.3714, cover loss of 0.1333, secret loss of 0.2380\n",
      "Training: Batch 50/104. Loss of 0.3329, cover loss of 0.1882, secret loss of 0.1447\n",
      "Training: Batch 51/104. Loss of 0.2553, cover loss of 0.1343, secret loss of 0.1210\n",
      "Training: Batch 52/104. Loss of 0.5948, cover loss of 0.2983, secret loss of 0.2965\n",
      "Training: Batch 53/104. Loss of 0.4646, cover loss of 0.2017, secret loss of 0.2629\n",
      "Training: Batch 54/104. Loss of 0.3709, cover loss of 0.1863, secret loss of 0.1846\n",
      "Training: Batch 55/104. Loss of 0.3014, cover loss of 0.1654, secret loss of 0.1360\n",
      "Training: Batch 56/104. Loss of 0.3396, cover loss of 0.1708, secret loss of 0.1688\n",
      "Training: Batch 57/104. Loss of 0.4861, cover loss of 0.2184, secret loss of 0.2677\n",
      "Training: Batch 58/104. Loss of 0.4112, cover loss of 0.1907, secret loss of 0.2205\n",
      "Training: Batch 59/104. Loss of 0.2982, cover loss of 0.1165, secret loss of 0.1817\n",
      "Training: Batch 60/104. Loss of 0.3775, cover loss of 0.1391, secret loss of 0.2384\n",
      "Training: Batch 61/104. Loss of 0.4510, cover loss of 0.2705, secret loss of 0.1805\n",
      "Training: Batch 62/104. Loss of 0.2411, cover loss of 0.1256, secret loss of 0.1155\n",
      "Training: Batch 63/104. Loss of 0.4707, cover loss of 0.1676, secret loss of 0.3031\n",
      "Training: Batch 64/104. Loss of 0.4262, cover loss of 0.2774, secret loss of 0.1488\n",
      "Training: Batch 65/104. Loss of 0.4686, cover loss of 0.2342, secret loss of 0.2344\n",
      "Training: Batch 66/104. Loss of 0.4952, cover loss of 0.2438, secret loss of 0.2514\n",
      "Training: Batch 67/104. Loss of 0.3203, cover loss of 0.1592, secret loss of 0.1611\n",
      "Training: Batch 68/104. Loss of 0.3159, cover loss of 0.1220, secret loss of 0.1938\n",
      "Training: Batch 69/104. Loss of 0.3659, cover loss of 0.1636, secret loss of 0.2023\n",
      "Training: Batch 70/104. Loss of 0.4934, cover loss of 0.2815, secret loss of 0.2119\n",
      "Training: Batch 71/104. Loss of 0.2793, cover loss of 0.1226, secret loss of 0.1567\n",
      "Training: Batch 72/104. Loss of 0.5612, cover loss of 0.2698, secret loss of 0.2914\n",
      "Training: Batch 73/104. Loss of 0.3014, cover loss of 0.1904, secret loss of 0.1110\n",
      "Training: Batch 74/104. Loss of 0.3205, cover loss of 0.1438, secret loss of 0.1767\n",
      "Training: Batch 75/104. Loss of 0.3405, cover loss of 0.1332, secret loss of 0.2074\n",
      "Training: Batch 76/104. Loss of 0.3703, cover loss of 0.1546, secret loss of 0.2157\n",
      "Training: Batch 77/104. Loss of 0.5069, cover loss of 0.1825, secret loss of 0.3244\n",
      "Training: Batch 78/104. Loss of 0.2670, cover loss of 0.1491, secret loss of 0.1179\n",
      "Training: Batch 79/104. Loss of 0.4579, cover loss of 0.1772, secret loss of 0.2808\n",
      "Training: Batch 80/104. Loss of 0.4071, cover loss of 0.1245, secret loss of 0.2827\n",
      "Training: Batch 81/104. Loss of 0.3775, cover loss of 0.1504, secret loss of 0.2271\n",
      "Training: Batch 82/104. Loss of 0.2941, cover loss of 0.1422, secret loss of 0.1518\n",
      "Training: Batch 83/104. Loss of 0.3412, cover loss of 0.1222, secret loss of 0.2189\n",
      "Training: Batch 84/104. Loss of 0.3624, cover loss of 0.1433, secret loss of 0.2191\n",
      "Training: Batch 85/104. Loss of 0.3811, cover loss of 0.1555, secret loss of 0.2257\n",
      "Training: Batch 86/104. Loss of 0.3095, cover loss of 0.1244, secret loss of 0.1850\n",
      "Training: Batch 87/104. Loss of 0.3996, cover loss of 0.1918, secret loss of 0.2078\n",
      "Training: Batch 88/104. Loss of 0.3818, cover loss of 0.1474, secret loss of 0.2344\n",
      "Training: Batch 89/104. Loss of 0.3733, cover loss of 0.1179, secret loss of 0.2553\n",
      "Training: Batch 90/104. Loss of 0.3755, cover loss of 0.1946, secret loss of 0.1808\n",
      "Training: Batch 91/104. Loss of 0.3605, cover loss of 0.1304, secret loss of 0.2300\n",
      "Training: Batch 92/104. Loss of 0.2688, cover loss of 0.0877, secret loss of 0.1811\n",
      "Training: Batch 93/104. Loss of 0.2944, cover loss of 0.1111, secret loss of 0.1833\n",
      "Training: Batch 94/104. Loss of 0.4061, cover loss of 0.1341, secret loss of 0.2719\n",
      "Training: Batch 95/104. Loss of 0.3501, cover loss of 0.0785, secret loss of 0.2716\n",
      "Training: Batch 96/104. Loss of 0.3046, cover loss of 0.1025, secret loss of 0.2021\n",
      "Training: Batch 97/104. Loss of 0.2433, cover loss of 0.0898, secret loss of 0.1535\n",
      "Training: Batch 98/104. Loss of 0.2665, cover loss of 0.0864, secret loss of 0.1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 99/104. Loss of 0.4030, cover loss of 0.1428, secret loss of 0.2602\n",
      "Training: Batch 100/104. Loss of 0.3068, cover loss of 0.0958, secret loss of 0.2111\n",
      "Training: Batch 101/104. Loss of 0.2789, cover loss of 0.0681, secret loss of 0.2108\n",
      "Training: Batch 102/104. Loss of 0.2577, cover loss of 0.0749, secret loss of 0.1828\n",
      "Training: Batch 103/104. Loss of 0.3903, cover loss of 0.1351, secret loss of 0.2552\n",
      "Training: Batch 104/104. Loss of 0.2128, cover loss of 0.0880, secret loss of 0.1248\n",
      "Epoch [1/3], Average_loss: 0.4893\n",
      "Training: Batch 1/104. Loss of 0.2275, cover loss of 0.0656, secret loss of 0.1618\n",
      "Training: Batch 2/104. Loss of 0.5195, cover loss of 0.1178, secret loss of 0.4017\n",
      "Training: Batch 3/104. Loss of 0.2745, cover loss of 0.0624, secret loss of 0.2121\n",
      "Training: Batch 4/104. Loss of 0.3304, cover loss of 0.0722, secret loss of 0.2582\n",
      "Training: Batch 5/104. Loss of 0.2522, cover loss of 0.0794, secret loss of 0.1727\n",
      "Training: Batch 6/104. Loss of 0.3553, cover loss of 0.0644, secret loss of 0.2909\n",
      "Training: Batch 7/104. Loss of 0.2647, cover loss of 0.0811, secret loss of 0.1836\n",
      "Training: Batch 8/104. Loss of 0.4232, cover loss of 0.0775, secret loss of 0.3457\n",
      "Training: Batch 9/104. Loss of 0.3451, cover loss of 0.0669, secret loss of 0.2781\n",
      "Training: Batch 10/104. Loss of 0.2666, cover loss of 0.0767, secret loss of 0.1899\n",
      "Training: Batch 11/104. Loss of 0.2803, cover loss of 0.0654, secret loss of 0.2149\n",
      "Training: Batch 12/104. Loss of 0.1762, cover loss of 0.0674, secret loss of 0.1088\n",
      "Training: Batch 13/104. Loss of 0.3595, cover loss of 0.1295, secret loss of 0.2300\n",
      "Training: Batch 14/104. Loss of 0.2711, cover loss of 0.0838, secret loss of 0.1873\n",
      "Training: Batch 15/104. Loss of 0.3164, cover loss of 0.0701, secret loss of 0.2463\n",
      "Training: Batch 16/104. Loss of 0.2731, cover loss of 0.0633, secret loss of 0.2098\n",
      "Training: Batch 17/104. Loss of 0.3885, cover loss of 0.0806, secret loss of 0.3079\n",
      "Training: Batch 18/104. Loss of 0.3325, cover loss of 0.0712, secret loss of 0.2613\n",
      "Training: Batch 19/104. Loss of 0.2557, cover loss of 0.0534, secret loss of 0.2022\n",
      "Training: Batch 20/104. Loss of 0.2595, cover loss of 0.0656, secret loss of 0.1939\n",
      "Training: Batch 21/104. Loss of 0.3541, cover loss of 0.0725, secret loss of 0.2815\n",
      "Training: Batch 22/104. Loss of 0.3987, cover loss of 0.0918, secret loss of 0.3068\n",
      "Training: Batch 23/104. Loss of 0.2468, cover loss of 0.0631, secret loss of 0.1836\n",
      "Training: Batch 24/104. Loss of 0.3279, cover loss of 0.0976, secret loss of 0.2303\n",
      "Training: Batch 25/104. Loss of 0.2400, cover loss of 0.0926, secret loss of 0.1474\n",
      "Training: Batch 26/104. Loss of 0.2364, cover loss of 0.0703, secret loss of 0.1661\n",
      "Training: Batch 27/104. Loss of 0.2695, cover loss of 0.0707, secret loss of 0.1988\n",
      "Training: Batch 28/104. Loss of 0.3854, cover loss of 0.0596, secret loss of 0.3257\n",
      "Training: Batch 29/104. Loss of 0.4185, cover loss of 0.0944, secret loss of 0.3241\n",
      "Training: Batch 30/104. Loss of 0.1815, cover loss of 0.0679, secret loss of 0.1136\n",
      "Training: Batch 31/104. Loss of 0.2709, cover loss of 0.0724, secret loss of 0.1985\n",
      "Training: Batch 32/104. Loss of 0.3126, cover loss of 0.0500, secret loss of 0.2626\n",
      "Training: Batch 33/104. Loss of 0.3030, cover loss of 0.0562, secret loss of 0.2468\n",
      "Training: Batch 34/104. Loss of 0.2417, cover loss of 0.0562, secret loss of 0.1855\n",
      "Training: Batch 35/104. Loss of 0.3602, cover loss of 0.0811, secret loss of 0.2791\n",
      "Training: Batch 36/104. Loss of 0.3171, cover loss of 0.0648, secret loss of 0.2522\n",
      "Training: Batch 37/104. Loss of 0.3905, cover loss of 0.0698, secret loss of 0.3207\n",
      "Training: Batch 38/104. Loss of 0.3271, cover loss of 0.0623, secret loss of 0.2647\n",
      "Training: Batch 39/104. Loss of 0.2607, cover loss of 0.0568, secret loss of 0.2039\n",
      "Training: Batch 40/104. Loss of 0.3375, cover loss of 0.0930, secret loss of 0.2446\n",
      "Training: Batch 41/104. Loss of 0.2197, cover loss of 0.0520, secret loss of 0.1677\n",
      "Training: Batch 42/104. Loss of 0.2055, cover loss of 0.0546, secret loss of 0.1510\n",
      "Training: Batch 43/104. Loss of 0.2911, cover loss of 0.0555, secret loss of 0.2356\n",
      "Training: Batch 44/104. Loss of 0.2252, cover loss of 0.0597, secret loss of 0.1655\n",
      "Training: Batch 45/104. Loss of 0.2900, cover loss of 0.0478, secret loss of 0.2422\n",
      "Training: Batch 46/104. Loss of 0.1763, cover loss of 0.0587, secret loss of 0.1176\n",
      "Training: Batch 47/104. Loss of 0.2329, cover loss of 0.0554, secret loss of 0.1774\n",
      "Training: Batch 48/104. Loss of 0.2190, cover loss of 0.0582, secret loss of 0.1608\n",
      "Training: Batch 49/104. Loss of 0.3389, cover loss of 0.0596, secret loss of 0.2793\n",
      "Training: Batch 50/104. Loss of 0.2299, cover loss of 0.0636, secret loss of 0.1663\n",
      "Training: Batch 51/104. Loss of 0.1732, cover loss of 0.0515, secret loss of 0.1216\n",
      "Training: Batch 52/104. Loss of 0.4581, cover loss of 0.1057, secret loss of 0.3525\n",
      "Training: Batch 53/104. Loss of 0.4037, cover loss of 0.0602, secret loss of 0.3435\n",
      "Training: Batch 54/104. Loss of 0.2423, cover loss of 0.0499, secret loss of 0.1924\n",
      "Training: Batch 55/104. Loss of 0.1740, cover loss of 0.0506, secret loss of 0.1234\n",
      "Training: Batch 56/104. Loss of 0.2506, cover loss of 0.0612, secret loss of 0.1894\n",
      "Training: Batch 57/104. Loss of 0.4027, cover loss of 0.0996, secret loss of 0.3031\n",
      "Training: Batch 58/104. Loss of 0.2577, cover loss of 0.0562, secret loss of 0.2015\n",
      "Training: Batch 59/104. Loss of 0.2305, cover loss of 0.0509, secret loss of 0.1796\n",
      "Training: Batch 60/104. Loss of 0.3092, cover loss of 0.0476, secret loss of 0.2616\n",
      "Training: Batch 61/104. Loss of 0.3387, cover loss of 0.0629, secret loss of 0.2758\n",
      "Training: Batch 62/104. Loss of 0.2089, cover loss of 0.0669, secret loss of 0.1420\n",
      "Training: Batch 63/104. Loss of 0.4058, cover loss of 0.0447, secret loss of 0.3611\n",
      "Training: Batch 64/104. Loss of 0.2520, cover loss of 0.0846, secret loss of 0.1674\n",
      "Training: Batch 65/104. Loss of 0.3503, cover loss of 0.1226, secret loss of 0.2277\n",
      "Training: Batch 66/104. Loss of 0.3622, cover loss of 0.0626, secret loss of 0.2995\n",
      "Training: Batch 67/104. Loss of 0.2277, cover loss of 0.0469, secret loss of 0.1808\n",
      "Training: Batch 68/104. Loss of 0.2240, cover loss of 0.0315, secret loss of 0.1925\n",
      "Training: Batch 69/104. Loss of 0.2730, cover loss of 0.0683, secret loss of 0.2047\n",
      "Training: Batch 70/104. Loss of 0.3417, cover loss of 0.0715, secret loss of 0.2702\n",
      "Training: Batch 71/104. Loss of 0.2121, cover loss of 0.0447, secret loss of 0.1674\n",
      "Training: Batch 72/104. Loss of 0.4802, cover loss of 0.0665, secret loss of 0.4137\n",
      "Training: Batch 73/104. Loss of 0.2147, cover loss of 0.0541, secret loss of 0.1606\n",
      "Training: Batch 74/104. Loss of 0.2571, cover loss of 0.0728, secret loss of 0.1843\n",
      "Training: Batch 75/104. Loss of 0.2749, cover loss of 0.0500, secret loss of 0.2249\n",
      "Training: Batch 76/104. Loss of 0.2860, cover loss of 0.0561, secret loss of 0.2299\n",
      "Training: Batch 77/104. Loss of 0.4266, cover loss of 0.0482, secret loss of 0.3784\n",
      "Training: Batch 78/104. Loss of 0.1693, cover loss of 0.0447, secret loss of 0.1246\n",
      "Training: Batch 79/104. Loss of 0.3718, cover loss of 0.0529, secret loss of 0.3189\n",
      "Training: Batch 80/104. Loss of 0.3344, cover loss of 0.0454, secret loss of 0.2890\n",
      "Training: Batch 81/104. Loss of 0.2852, cover loss of 0.0592, secret loss of 0.2260\n",
      "Training: Batch 82/104. Loss of 0.2297, cover loss of 0.0491, secret loss of 0.1806\n",
      "Training: Batch 83/104. Loss of 0.2731, cover loss of 0.0451, secret loss of 0.2280\n",
      "Training: Batch 84/104. Loss of 0.2688, cover loss of 0.0393, secret loss of 0.2294\n",
      "Training: Batch 85/104. Loss of 0.2987, cover loss of 0.0659, secret loss of 0.2328\n",
      "Training: Batch 86/104. Loss of 0.2254, cover loss of 0.0430, secret loss of 0.1824\n",
      "Training: Batch 87/104. Loss of 0.2953, cover loss of 0.0940, secret loss of 0.2013\n",
      "Training: Batch 88/104. Loss of 0.2401, cover loss of 0.0630, secret loss of 0.1770\n",
      "Training: Batch 89/104. Loss of 0.2818, cover loss of 0.0559, secret loss of 0.2260\n",
      "Training: Batch 90/104. Loss of 0.2954, cover loss of 0.0925, secret loss of 0.2028\n",
      "Training: Batch 91/104. Loss of 0.2490, cover loss of 0.0534, secret loss of 0.1956\n",
      "Training: Batch 92/104. Loss of 0.2202, cover loss of 0.0519, secret loss of 0.1684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 93/104. Loss of 0.2626, cover loss of 0.0431, secret loss of 0.2196\n",
      "Training: Batch 94/104. Loss of 0.3636, cover loss of 0.0900, secret loss of 0.2736\n",
      "Training: Batch 95/104. Loss of 0.3093, cover loss of 0.0571, secret loss of 0.2522\n",
      "Training: Batch 96/104. Loss of 0.2005, cover loss of 0.0455, secret loss of 0.1551\n",
      "Training: Batch 97/104. Loss of 0.1832, cover loss of 0.0526, secret loss of 0.1306\n",
      "Training: Batch 98/104. Loss of 0.2429, cover loss of 0.0671, secret loss of 0.1758\n",
      "Training: Batch 99/104. Loss of 0.3421, cover loss of 0.1128, secret loss of 0.2292\n",
      "Training: Batch 100/104. Loss of 0.2220, cover loss of 0.0645, secret loss of 0.1575\n",
      "Training: Batch 101/104. Loss of 0.2439, cover loss of 0.0474, secret loss of 0.1965\n",
      "Training: Batch 102/104. Loss of 0.2160, cover loss of 0.0477, secret loss of 0.1683\n",
      "Training: Batch 103/104. Loss of 0.2935, cover loss of 0.1080, secret loss of 0.1855\n",
      "Training: Batch 104/104. Loss of 0.1883, cover loss of 0.0611, secret loss of 0.1272\n",
      "Epoch [2/3], Average_loss: 0.2877\n",
      "Training: Batch 1/104. Loss of 0.1713, cover loss of 0.0378, secret loss of 0.1336\n",
      "Training: Batch 2/104. Loss of 0.4046, cover loss of 0.0743, secret loss of 0.3303\n",
      "Training: Batch 3/104. Loss of 0.2378, cover loss of 0.0461, secret loss of 0.1918\n",
      "Training: Batch 4/104. Loss of 0.2976, cover loss of 0.0452, secret loss of 0.2523\n",
      "Training: Batch 5/104. Loss of 0.1986, cover loss of 0.0457, secret loss of 0.1529\n",
      "Training: Batch 6/104. Loss of 0.3272, cover loss of 0.0447, secret loss of 0.2826\n",
      "Training: Batch 7/104. Loss of 0.2348, cover loss of 0.0498, secret loss of 0.1850\n",
      "Training: Batch 8/104. Loss of 0.3924, cover loss of 0.0498, secret loss of 0.3426\n",
      "Training: Batch 9/104. Loss of 0.2970, cover loss of 0.0433, secret loss of 0.2537\n",
      "Training: Batch 10/104. Loss of 0.2275, cover loss of 0.0517, secret loss of 0.1758\n",
      "Training: Batch 11/104. Loss of 0.2559, cover loss of 0.0434, secret loss of 0.2126\n",
      "Training: Batch 12/104. Loss of 0.1575, cover loss of 0.0488, secret loss of 0.1088\n",
      "Training: Batch 13/104. Loss of 0.3242, cover loss of 0.1054, secret loss of 0.2187\n",
      "Training: Batch 14/104. Loss of 0.2327, cover loss of 0.0613, secret loss of 0.1714\n",
      "Training: Batch 15/104. Loss of 0.2859, cover loss of 0.0497, secret loss of 0.2363\n",
      "Training: Batch 16/104. Loss of 0.2211, cover loss of 0.0364, secret loss of 0.1847\n",
      "Training: Batch 17/104. Loss of 0.3538, cover loss of 0.0506, secret loss of 0.3032\n",
      "Training: Batch 18/104. Loss of 0.2938, cover loss of 0.0486, secret loss of 0.2452\n",
      "Training: Batch 19/104. Loss of 0.2336, cover loss of 0.0358, secret loss of 0.1978\n",
      "Training: Batch 20/104. Loss of 0.2809, cover loss of 0.0423, secret loss of 0.2386\n",
      "Training: Batch 21/104. Loss of 0.3407, cover loss of 0.0466, secret loss of 0.2941\n",
      "Training: Batch 22/104. Loss of 0.3940, cover loss of 0.0693, secret loss of 0.3247\n",
      "Training: Batch 23/104. Loss of 0.2100, cover loss of 0.0416, secret loss of 0.1684\n",
      "Training: Batch 24/104. Loss of 0.2761, cover loss of 0.0748, secret loss of 0.2013\n",
      "Training: Batch 25/104. Loss of 0.2049, cover loss of 0.0698, secret loss of 0.1350\n",
      "Training: Batch 26/104. Loss of 0.2167, cover loss of 0.0496, secret loss of 0.1671\n",
      "Training: Batch 27/104. Loss of 0.2457, cover loss of 0.0518, secret loss of 0.1939\n",
      "Training: Batch 28/104. Loss of 0.3619, cover loss of 0.0413, secret loss of 0.3206\n",
      "Training: Batch 29/104. Loss of 0.3966, cover loss of 0.0767, secret loss of 0.3198\n",
      "Training: Batch 30/104. Loss of 0.1701, cover loss of 0.0537, secret loss of 0.1163\n",
      "Training: Batch 31/104. Loss of 0.2560, cover loss of 0.0557, secret loss of 0.2004\n",
      "Training: Batch 32/104. Loss of 0.3115, cover loss of 0.0348, secret loss of 0.2766\n",
      "Training: Batch 33/104. Loss of 0.2941, cover loss of 0.0405, secret loss of 0.2536\n",
      "Training: Batch 34/104. Loss of 0.2040, cover loss of 0.0388, secret loss of 0.1652\n",
      "Training: Batch 35/104. Loss of 0.3389, cover loss of 0.0573, secret loss of 0.2816\n",
      "Training: Batch 36/104. Loss of 0.2666, cover loss of 0.0446, secret loss of 0.2219\n",
      "Training: Batch 37/104. Loss of 0.3570, cover loss of 0.0495, secret loss of 0.3074\n",
      "Training: Batch 38/104. Loss of 0.3149, cover loss of 0.0465, secret loss of 0.2685\n",
      "Training: Batch 39/104. Loss of 0.2432, cover loss of 0.0409, secret loss of 0.2023\n",
      "Training: Batch 40/104. Loss of 0.3086, cover loss of 0.0745, secret loss of 0.2341\n",
      "Training: Batch 41/104. Loss of 0.1887, cover loss of 0.0365, secret loss of 0.1522\n",
      "Training: Batch 42/104. Loss of 0.1765, cover loss of 0.0376, secret loss of 0.1389\n",
      "Training: Batch 43/104. Loss of 0.2814, cover loss of 0.0411, secret loss of 0.2403\n",
      "Training: Batch 44/104. Loss of 0.2017, cover loss of 0.0449, secret loss of 0.1568\n",
      "Training: Batch 45/104. Loss of 0.2690, cover loss of 0.0349, secret loss of 0.2341\n",
      "Training: Batch 46/104. Loss of 0.1596, cover loss of 0.0376, secret loss of 0.1220\n",
      "Training: Batch 47/104. Loss of 0.2066, cover loss of 0.0392, secret loss of 0.1673\n",
      "Training: Batch 48/104. Loss of 0.2003, cover loss of 0.0405, secret loss of 0.1598\n",
      "Training: Batch 49/104. Loss of 0.3247, cover loss of 0.0443, secret loss of 0.2804\n",
      "Training: Batch 50/104. Loss of 0.2151, cover loss of 0.0458, secret loss of 0.1693\n",
      "Training: Batch 51/104. Loss of 0.1577, cover loss of 0.0360, secret loss of 0.1217\n",
      "Training: Batch 52/104. Loss of 0.4452, cover loss of 0.0870, secret loss of 0.3582\n",
      "Training: Batch 53/104. Loss of 0.3674, cover loss of 0.0453, secret loss of 0.3221\n",
      "Training: Batch 54/104. Loss of 0.2258, cover loss of 0.0357, secret loss of 0.1901\n",
      "Training: Batch 55/104. Loss of 0.1536, cover loss of 0.0366, secret loss of 0.1170\n",
      "Training: Batch 56/104. Loss of 0.2304, cover loss of 0.0459, secret loss of 0.1845\n",
      "Training: Batch 57/104. Loss of 0.3959, cover loss of 0.0844, secret loss of 0.3115\n",
      "Training: Batch 58/104. Loss of 0.2460, cover loss of 0.0445, secret loss of 0.2015\n",
      "Training: Batch 59/104. Loss of 0.2161, cover loss of 0.0359, secret loss of 0.1801\n",
      "Training: Batch 60/104. Loss of 0.2922, cover loss of 0.0373, secret loss of 0.2550\n",
      "Training: Batch 61/104. Loss of 0.3143, cover loss of 0.0509, secret loss of 0.2634\n",
      "Training: Batch 62/104. Loss of 0.1775, cover loss of 0.0561, secret loss of 0.1215\n",
      "Training: Batch 63/104. Loss of 0.3798, cover loss of 0.0290, secret loss of 0.3508\n",
      "Training: Batch 64/104. Loss of 0.2319, cover loss of 0.0714, secret loss of 0.1605\n",
      "Training: Batch 65/104. Loss of 0.3390, cover loss of 0.1079, secret loss of 0.2311\n",
      "Training: Batch 66/104. Loss of 0.3415, cover loss of 0.0429, secret loss of 0.2986\n",
      "Training: Batch 67/104. Loss of 0.2148, cover loss of 0.0308, secret loss of 0.1840\n",
      "Training: Batch 68/104. Loss of 0.2140, cover loss of 0.0224, secret loss of 0.1916\n",
      "Training: Batch 69/104. Loss of 0.2643, cover loss of 0.0546, secret loss of 0.2097\n",
      "Training: Batch 70/104. Loss of 0.3072, cover loss of 0.0506, secret loss of 0.2566\n",
      "Training: Batch 71/104. Loss of 0.2044, cover loss of 0.0333, secret loss of 0.1711\n",
      "Training: Batch 72/104. Loss of 0.4751, cover loss of 0.0513, secret loss of 0.4238\n",
      "Training: Batch 73/104. Loss of 0.2130, cover loss of 0.0434, secret loss of 0.1696\n",
      "Training: Batch 74/104. Loss of 0.2395, cover loss of 0.0510, secret loss of 0.1885\n",
      "Training: Batch 75/104. Loss of 0.2826, cover loss of 0.0409, secret loss of 0.2417\n",
      "Training: Batch 76/104. Loss of 0.2601, cover loss of 0.0445, secret loss of 0.2156\n",
      "Training: Batch 77/104. Loss of 0.4052, cover loss of 0.0329, secret loss of 0.3724\n",
      "Training: Batch 78/104. Loss of 0.1569, cover loss of 0.0336, secret loss of 0.1233\n",
      "Training: Batch 79/104. Loss of 0.3414, cover loss of 0.0346, secret loss of 0.3068\n",
      "Training: Batch 80/104. Loss of 0.3239, cover loss of 0.0364, secret loss of 0.2875\n",
      "Training: Batch 81/104. Loss of 0.2735, cover loss of 0.0441, secret loss of 0.2294\n",
      "Training: Batch 82/104. Loss of 0.2005, cover loss of 0.0358, secret loss of 0.1648\n",
      "Training: Batch 83/104. Loss of 0.2635, cover loss of 0.0304, secret loss of 0.2331\n",
      "Training: Batch 84/104. Loss of 0.2574, cover loss of 0.0294, secret loss of 0.2279\n",
      "Training: Batch 85/104. Loss of 0.2879, cover loss of 0.0531, secret loss of 0.2348\n",
      "Training: Batch 86/104. Loss of 0.2170, cover loss of 0.0316, secret loss of 0.1854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 87/104. Loss of 0.2813, cover loss of 0.0797, secret loss of 0.2015\n",
      "Training: Batch 88/104. Loss of 0.2281, cover loss of 0.0520, secret loss of 0.1761\n",
      "Training: Batch 89/104. Loss of 0.2678, cover loss of 0.0433, secret loss of 0.2246\n",
      "Training: Batch 90/104. Loss of 0.3013, cover loss of 0.0764, secret loss of 0.2249\n",
      "Training: Batch 91/104. Loss of 0.2310, cover loss of 0.0377, secret loss of 0.1933\n",
      "Training: Batch 92/104. Loss of 0.1999, cover loss of 0.0342, secret loss of 0.1657\n",
      "Training: Batch 93/104. Loss of 0.2499, cover loss of 0.0307, secret loss of 0.2192\n",
      "Training: Batch 94/104. Loss of 0.3496, cover loss of 0.0728, secret loss of 0.2768\n",
      "Training: Batch 95/104. Loss of 0.2583, cover loss of 0.0305, secret loss of 0.2277\n",
      "Training: Batch 96/104. Loss of 0.1851, cover loss of 0.0348, secret loss of 0.1503\n",
      "Training: Batch 97/104. Loss of 0.1604, cover loss of 0.0340, secret loss of 0.1264\n",
      "Training: Batch 98/104. Loss of 0.2285, cover loss of 0.0526, secret loss of 0.1759\n",
      "Training: Batch 99/104. Loss of 0.3220, cover loss of 0.0934, secret loss of 0.2286\n",
      "Training: Batch 100/104. Loss of 0.2082, cover loss of 0.0508, secret loss of 0.1574\n",
      "Training: Batch 101/104. Loss of 0.2278, cover loss of 0.0309, secret loss of 0.1969\n",
      "Training: Batch 102/104. Loss of 0.2083, cover loss of 0.0389, secret loss of 0.1693\n",
      "Training: Batch 103/104. Loss of 0.2790, cover loss of 0.0915, secret loss of 0.1875\n",
      "Training: Batch 104/104. Loss of 0.1730, cover loss of 0.0417, secret loss of 0.1313\n",
      "Epoch [3/3], Average_loss: 0.2658\n"
     ]
    }
   ],
   "source": [
    "net, mean_train_loss, loss_history = train_model(train_set, train_set_secret, train_set_cover, beta, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-f1e584a7a283>:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_secret = Variable(test_secret, volatile=True).to(device)\n",
      "<ipython-input-15-f1e584a7a283>:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_cover = Variable(test_cover, volatile=True).to(device)\n",
      "<ipython-input-8-40fe894c6fe5>:116: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = torch.nn.init.normal(torch.Tensor(out.data.size()), 0, 0.1).to(device)\n",
      "<ipython-input-10-95a7298b6f53>:4: UserWarning: Using a target size (torch.Size([3, 1, 256, 256])) that is different to the input size (torch.Size([3, 3, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_cover = torch.sqrt(torch.nn.functional.mse_loss(C_prime, C))\n",
      "<ipython-input-10-95a7298b6f53>:5: UserWarning: Using a target size (torch.Size([3, 1, 256, 256])) that is different to the input size (torch.Size([3, 3, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_secret = torch.sqrt(torch.nn.functional.mse_loss(S_prime, S))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: photo 1/26 loss: 0.2233453393 \n",
      "Loss on secret: 0.1737899482 \n",
      "Loss on cover: 0.0495553985\n",
      "Validation: photo 2/26 loss: 0.4313995838 \n",
      "Loss on secret: 0.3112471104 \n",
      "Loss on cover: 0.1201524809\n",
      "Validation: photo 3/26 loss: 0.2980785072 \n",
      "Loss on secret: 0.2582776546 \n",
      "Loss on cover: 0.0398008488\n",
      "Validation: photo 4/26 loss: 0.4384064972 \n",
      "Loss on secret: 0.3591043949 \n",
      "Loss on cover: 0.0793021098\n",
      "Validation: photo 5/26 loss: 0.1590977311 \n",
      "Loss on secret: 0.1223157421 \n",
      "Loss on cover: 0.0367819928\n",
      "Validation: photo 6/26 loss: 0.2453647256 \n",
      "Loss on secret: 0.2121194452 \n",
      "Loss on cover: 0.0332452729\n",
      "Validation: photo 7/26 loss: 0.1837298423 \n",
      "Loss on secret: 0.1554396749 \n",
      "Loss on cover: 0.0282901675\n",
      "Validation: photo 8/26 loss: 0.1827267706 \n",
      "Loss on secret: 0.1403833032 \n",
      "Loss on cover: 0.0423434600\n",
      "Validation: photo 9/26 loss: 0.1614921540 \n",
      "Loss on secret: 0.1115966812 \n",
      "Loss on cover: 0.0498954691\n",
      "Validation: photo 10/26 loss: 0.2308148891 \n",
      "Loss on secret: 0.1982727945 \n",
      "Loss on cover: 0.0325420983\n",
      "Validation: photo 11/26 loss: 0.1751139909 \n",
      "Loss on secret: 0.1372976005 \n",
      "Loss on cover: 0.0378163941\n",
      "Validation: photo 12/26 loss: 0.1669808626 \n",
      "Loss on secret: 0.1366867721 \n",
      "Loss on cover: 0.0302940942\n",
      "Validation: photo 13/26 loss: 0.2500353158 \n",
      "Loss on secret: 0.1595178992 \n",
      "Loss on cover: 0.0905174240\n",
      "Validation: photo 14/26 loss: 0.3790550232 \n",
      "Loss on secret: 0.3524245322 \n",
      "Loss on cover: 0.0266304947\n",
      "Validation: photo 15/26 loss: 0.3302143514 \n",
      "Loss on secret: 0.3036930263 \n",
      "Loss on cover: 0.0265213344\n",
      "Validation: photo 16/26 loss: 0.1643384099 \n",
      "Loss on secret: 0.1367954910 \n",
      "Loss on cover: 0.0275429171\n",
      "Validation: photo 17/26 loss: 0.2856429815 \n",
      "Loss on secret: 0.2445374280 \n",
      "Loss on cover: 0.0411055423\n",
      "Validation: photo 18/26 loss: 0.1690055281 \n",
      "Loss on secret: 0.1430408210 \n",
      "Loss on cover: 0.0259647090\n",
      "Validation: photo 19/26 loss: 0.3065094650 \n",
      "Loss on secret: 0.2758303881 \n",
      "Loss on cover: 0.0306790639\n",
      "Validation: photo 20/26 loss: 0.3638405502 \n",
      "Loss on secret: 0.3363820314 \n",
      "Loss on cover: 0.0274585262\n",
      "Validation: photo 21/26 loss: 0.1754046977 \n",
      "Loss on secret: 0.1428929418 \n",
      "Loss on cover: 0.0325117521\n",
      "Validation: photo 22/26 loss: 0.3234201372 \n",
      "Loss on secret: 0.2968372703 \n",
      "Loss on cover: 0.0265828576\n",
      "Validation: photo 23/26 loss: 0.2293616682 \n",
      "Loss on secret: 0.1926742345 \n",
      "Loss on cover: 0.0366874337\n",
      "Validation: photo 24/26 loss: 0.2251868248 \n",
      "Loss on secret: 0.1857639700 \n",
      "Loss on cover: 0.0394228548\n",
      "Validation: photo 25/26 loss: 0.2170995027 \n",
      "Loss on secret: 0.1856764257 \n",
      "Loss on cover: 0.0314230770\n",
      "Validation: photo 26/26 loss: 0.1888956130 \n",
      "Loss on secret: 0.1401025653 \n",
      "Loss on cover: 0.0487930439\n",
      "Average Validation loss on validation set: 0.2501754463\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net.eval()\n",
    "if device == 'cuda':\n",
    "    pass\n",
    "else:\n",
    "    net.to(device)\n",
    "\n",
    "test_losses = []\n",
    "# Show images\n",
    "for idx in range(len(test_set_cover)):\n",
    "    test_secret = test_set_secret[idx].float().unsqueeze(1)\n",
    "    test_cover = test_set_cover[idx].float().unsqueeze(1)\n",
    "\n",
    "    # Creates variable from secret and cover images\n",
    "    test_secret = Variable(test_secret, volatile=True).to(device)\n",
    "    test_cover = Variable(test_cover, volatile=True).to(device)\n",
    "\n",
    "    # Compute output\n",
    "    test_hidden, test_output = net(test_secret, test_cover, device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        pass\n",
    "    else:\n",
    "        test_loss.to('cpu')\n",
    "        loss_cover.to('cpu')\n",
    "        loss_secret.to('cpu')\n",
    "    print ('Validation: photo {}/{} loss: {:.10f} \\nLoss on secret: {:.10f} \\nLoss on cover: {:.10f}'.format(idx+1, int(len(test_set_cover)), test_loss.data, loss_secret.data, loss_cover.data))\n",
    "        \n",
    "    test_losses.append(test_loss.data)\n",
    "        \n",
    "mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "print ('Average Validation loss on validation set: {:.10f}'.format(mean_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjhmFrmtyxAz"
   },
   "source": [
    "## Optional Task\n",
    "\n",
    "What if you have two secret images to encrpt, what if there are more. Does LSB work? Does the NN work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_cover_secret(train_set, test_set):\n",
    "  one_third_train = int(len(train_set) / 3)\n",
    "  train_set_cover = train_set[:one_third_train]\n",
    "  train_set_secret = train_set[one_third_train : 2 * one_third_train]\n",
    "  train_set_secret_2 = train_set[2 * one_third_train:]\n",
    "\n",
    "  one_third_test = int(len(test_set) / 3)\n",
    "  test_set_cover = test_set[:one_third_test]\n",
    "  test_set_secret = test_set[one_third_test : 2 * one_third_test]\n",
    "  test_set_secret_2 = test_set[2 * one_third_test:]\n",
    "  return train_set_cover, train_set_secret, train_set_secret_2, test_set_cover, test_set_secret, test_set_secret_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_cover, train_set_secret, train_set_secret_2, test_set_cover, test_set_secret, test_set_secret_2 = separate_cover_secret(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_loss(S_prime, S_2_prime, C_prime, S, S_2, C, B):\n",
    "    ''' Calculates loss specified on the paper.'''\n",
    "    \n",
    "    loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n",
    "    loss_secret = torch.nn.functional.mse_loss(S_prime, S)\n",
    "    loss_secret_2 = torch.nn.functional.mse_loss(S_2_prime, S_2)\n",
    "\n",
    "    loss_all = loss_cover + B * loss_secret + B * loss_secret_2\n",
    "    return loss_all, loss_cover, loss_secret, loss_secret_2\n",
    "\n",
    "def denormalize(image, std, mean):\n",
    "    ''' Denormalizes a tensor of images.'''\n",
    "\n",
    "    for t in range(3):\n",
    "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
    "    return image\n",
    "\n",
    "def imshow(img, idx, learning_rate, beta):\n",
    "    '''Prints out an image given in tensor format.'''\n",
    "    \n",
    "    img = denormalize(img, std, mean)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def gaussian(tensor, mean=0, stddev=0.1):\n",
    "    '''Adds random noise to a tensor.'''\n",
    "    \n",
    "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1).to('cuda')\n",
    "    return Variable(tensor + noise).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrepNetwork, self).__init__()\n",
    "        self.initialP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,50,kernel_size = 3, padding=1)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(50,50,kernel_size = 3, padding=1)),\n",
    "          ('relu2', nn.ReLU()),\n",
    "        ]))\n",
    "        self.initialP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv3', nn.Conv2d(1,50,kernel_size = 4, padding=1)),\n",
    "          ('relu3', nn.ReLU()),\n",
    "          ('conv4', nn.Conv2d(50,50,kernel_size = 4, padding=2)),\n",
    "          ('relu4', nn.ReLU()),\n",
    "        ]))\n",
    "        self.initialP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv5', nn.Conv2d(1,50,kernel_size = 5, padding=2)),\n",
    "          ('relu5', nn.ReLU()),\n",
    "          ('conv6', nn.Conv2d(50,50,kernel_size = 5, padding=2)),\n",
    "          ('relu6', nn.ReLU()),\n",
    "        ]))\n",
    "        self.finalP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv7', nn.Conv2d(150,50,kernel_size = 3, padding=1)),\n",
    "          ('relu7', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv8', nn.Conv2d(150,50,kernel_size = 4, padding=1)),\n",
    "          ('relu8', nn.ReLU()),\n",
    "          ('conv9', nn.Conv2d(50, 50, kernel_size=4, padding=2)),\n",
    "          ('relu9', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv10', nn.Conv2d(150,50, kernel_size = 5, padding=2)),\n",
    "          ('relu10', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "\n",
    "    def forward(self, p):\n",
    "        p1 = self.initialP3(p)\n",
    "        p2 = self.initialP4(p)\n",
    "        p3 = self.initialP5(p)\n",
    "        mid = torch.cat((p1, p2, p3), 1)\n",
    "        p4 = self.finalP3(mid)\n",
    "        p5 = self.finalP4(mid)\n",
    "        p6 = self.finalP5(mid)\n",
    "        out = torch.cat((p4, p5, p6), 1)\n",
    "        return out\n",
    "\n",
    "class PrepNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrepNetwork2, self).__init__()\n",
    "        self.initialP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv11', nn.Conv2d(1,50,kernel_size = 3, padding=1)),\n",
    "          ('relu11', nn.ReLU()),\n",
    "          ('conv12', nn.Conv2d(50,50,kernel_size = 3, padding=1)),\n",
    "          ('relu12', nn.ReLU()),\n",
    "        ]))\n",
    "        self.initialP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv13', nn.Conv2d(1,50,kernel_size = 4, padding=1)),\n",
    "          ('relu13', nn.ReLU()),\n",
    "          ('conv14', nn.Conv2d(50,50,kernel_size = 4, padding=2)),\n",
    "          ('relu14', nn.ReLU()),\n",
    "        ]))\n",
    "        self.initialP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv15', nn.Conv2d(1,50,kernel_size = 5, padding=2)),\n",
    "          ('relu15', nn.ReLU()),\n",
    "          ('conv16', nn.Conv2d(50,50,kernel_size = 5, padding=2)),\n",
    "          ('relu16', nn.ReLU()),\n",
    "        ]))\n",
    "        self.finalP3 = nn.Sequential(OrderedDict([\n",
    "          ('conv17', nn.Conv2d(150,50,kernel_size = 3, padding=1)),\n",
    "          ('relu17', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP4 = nn.Sequential(OrderedDict([\n",
    "          ('conv18', nn.Conv2d(150,50,kernel_size = 4, padding=1)),\n",
    "          ('relu18', nn.ReLU()),\n",
    "          ('conv19', nn.Conv2d(50, 50, kernel_size=4, padding=2)),\n",
    "          ('relu19', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalP5 = nn.Sequential(OrderedDict([\n",
    "          ('conv20', nn.Conv2d(150,50, kernel_size = 5, padding=2)),\n",
    "          ('relu20', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "\n",
    "    def forward(self, p_2):\n",
    "        p1_2 = self.initialP3(p_2)\n",
    "        p2_2 = self.initialP4(p_2)\n",
    "        p3_2 = self.initialP5(p_2)\n",
    "        mid_2 = torch.cat((p1_2, p2_2, p3_2), 1)\n",
    "        p4_2 = self.finalP3(mid_2)\n",
    "        p5_2 = self.finalP4(mid_2)\n",
    "        p6_2 = self.finalP5(mid_2)\n",
    "        out_2 = torch.cat((p4_2, p5_2, p6_2), 1)\n",
    "        return out_2\n",
    "\n",
    "# Hiding Network (5 conv layers)\n",
    "class HidingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HidingNetwork, self).__init__()\n",
    "        self.initialH3 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialH4 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialH5 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalH4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH = nn.Sequential(\n",
    "            nn.Conv2d(150, 1, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h1 = self.initialH3(h)\n",
    "        h2 = self.initialH4(h)\n",
    "        h3 = self.initialH5(h)\n",
    "        mid = torch.cat((h1, h2, h3), 1)\n",
    "        h4 = self.finalH3(mid)\n",
    "        h5 = self.finalH4(mid)\n",
    "        h6 = self.finalH5(mid)\n",
    "        mid2 = torch.cat((h4, h5, h6), 1)\n",
    "        out = self.finalH(mid2)\n",
    "        out_noise = gaussian(out.data, 0, 0.1)\n",
    "        return out, out_noise\n",
    "\n",
    "# Hiding Network 2 to hide second image\n",
    "class HidingNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HidingNetwork2, self).__init__()\n",
    "        self.initialH3 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialH4 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialH5 = nn.Sequential(\n",
    "            nn.Conv2d(151, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalH4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.finalH5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalH = nn.Sequential(\n",
    "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, h_2):\n",
    "        h1_2 = self.initialH3(h_2)\n",
    "        h2_2 = self.initialH4(h_2)\n",
    "        h3_2 = self.initialH5(h_2)\n",
    "        mid_2 = torch.cat((h1_2, h2_2, h3_2), 1)\n",
    "        h4_2 = self.finalH3(mid_2)\n",
    "        h5_2 = self.finalH4(mid_2)\n",
    "        h6_2 = self.finalH5(mid_2)\n",
    "        mid2_2 = torch.cat((h4_2, h5_2, h6_2), 1)\n",
    "        out_2 = self.finalH(mid2_2)\n",
    "        out_noise_2 = gaussian(out_2.data, 0, 0.1)\n",
    "        return out_2, out_noise_2\n",
    "\n",
    "# Reveal Network (2 conv layers)\n",
    "class RevealNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevealNetwork, self).__init__()\n",
    "        self.initialR3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialR4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialR5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalR4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR = nn.Sequential(\n",
    "            nn.Conv2d(150, 1, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, r):\n",
    "        r1 = self.initialR3(r)\n",
    "        r2 = self.initialR4(r)\n",
    "        r3 = self.initialR5(r)\n",
    "        mid = torch.cat((r1, r2, r3), 1)\n",
    "        r4 = self.finalR3(mid)\n",
    "        r5 = self.finalR4(mid)\n",
    "        r6 = self.finalR5(mid)\n",
    "        mid2 = torch.cat((r4, r5, r6), 1)\n",
    "        out = self.finalR(mid2)\n",
    "        return out\n",
    "\n",
    "# Reveal Network (2 conv layers)\n",
    "class RevealNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RevealNetwork2, self).__init__()\n",
    "        self.initialR3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.initialR4 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.initialR5 = nn.Sequential(\n",
    "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR3 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.finalR4 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR5 = nn.Sequential(\n",
    "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
    "            nn.ReLU())\n",
    "        self.finalR = nn.Sequential(\n",
    "            nn.Conv2d(150, 1, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, r_2):\n",
    "        r1_2 = self.initialR3(r_2)\n",
    "        r2_2 = self.initialR4(r_2)\n",
    "        r3_2 = self.initialR5(r_2)\n",
    "        mid_2 = torch.cat((r1_2, r2_2, r3_2), 1)\n",
    "        r4_2 = self.finalR3(mid_2)\n",
    "        r5_2 = self.finalR4(mid_2)\n",
    "        r6_2 = self.finalR5(mid_2)\n",
    "        mid2_2 = torch.cat((r4_2, r5_2, r6_2), 1)\n",
    "        out_2 = self.finalR(mid2_2)\n",
    "        return out_2\n",
    "\n",
    "# Join six networks in one module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.m1 = PrepNetwork()\n",
    "        self.m2 = PrepNetwork2()\n",
    "        self.m3 = HidingNetwork()\n",
    "        self.m4 = HidingNetwork2()\n",
    "        self.m5 = RevealNetwork()\n",
    "        self.m6 = RevealNetwork2()\n",
    "\n",
    "    def forward(self, secret, secret_2, cover):\n",
    "        x_1 = self.m1(secret)\n",
    "        x_2 = self.m2(secret_2)\n",
    "        mid = torch.cat((x_1, cover), 1)\n",
    "        x_3, x_3_noise = self.m3(mid)\n",
    "        mid2 = torch.cat((x_2, x_3), 1)\n",
    "        x_4, x_4_noise = self.m4(mid2)\n",
    "        x_5 = self.m5(x_4)\n",
    "        x_6 = self.m6(x_4)\n",
    "        return x_4,x_5,x_6\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optional(train_set, train_secrets, train_secrets_2, train_covers, beta, learning_rate, num_epochs):\n",
    "    # Save optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    # Iterate over batches performing forward and backward passes\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "        net.to('cuda')\n",
    "        #net.to('cuda')\n",
    "        train_losses = []\n",
    "        # Train one epoch\n",
    "        #for idx, res_lst in enumerate(train_set):\n",
    "        for idx in range(len(train_set_cover)):\n",
    "            train_covers = train_set_cover[idx].float().unsqueeze(1)\n",
    "            train_secrets = train_set_secret[idx].float().unsqueeze(1)\n",
    "            train_secrets_2 = train_set_secret_2[idx].float().unsqueeze(1)\n",
    "            #train_covers.to('cuda')\n",
    "            #train_secrets.to('cuda')\n",
    "\n",
    "            # Creates variable from secret and cover images\n",
    "            train_secrets = Variable(train_secrets, requires_grad=False).to('cuda')\n",
    "            train_secrets_2 = Variable(train_secrets_2, requires_grad=False).to('cuda')\n",
    "            train_covers = Variable(train_covers, requires_grad=False).to('cuda')\n",
    "            \n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_hidden, train_output, train_output_2 = net(train_secrets, train_secrets_2, train_covers)\n",
    "\n",
    "            # Calculate loss and perform backprop\n",
    "            train_loss, train_loss_cover, train_loss_secret, train_loss_secret_2 = customized_loss(train_output, train_output_2, train_hidden, train_secrets, train_secrets_2, train_covers, beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Saves training loss\n",
    "            train_loss.data.to('cpu')\n",
    "            train_loss_cover.data.to('cpu')\n",
    "            train_loss_secret.data.to('cpu')\n",
    "            train_loss_secret_2.data.to('cpu')\n",
    "            train_losses.append(train_loss.data.to('cpu'))\n",
    "            loss_history.append(train_loss.data.to('cpu'))\n",
    "            \n",
    "            # Prints mini-batch losses\n",
    "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret1 loss of {4:.4f}, secret2 loss of {5:4f}'.format(idx+1, int(len(train_set)/2), train_loss.data, train_loss_cover.data, train_loss_secret.data, train_loss_secret_2.data))\n",
    "    \n",
    "        torch.save(net.state_dict(), 'Epoch N{}.pkl'.format(epoch+1))\n",
    "        \n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "    \n",
    "        # Prints epoch average loss\n",
    "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
    "                epoch+1, num_epochs, mean_train_loss))\n",
    "    \n",
    "    return net, mean_train_loss, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-8bc6b9825158>:31: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1).to('cuda')\n",
      "<ipython-input-9-8bc6b9825158>:4: UserWarning: Using a target size (torch.Size([3, 1, 256, 256])) that is different to the input size (torch.Size([3, 3, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 1/104. Loss of 0.1574, cover loss of 0.0241, secret1 loss of 0.0788, secret2 loss of 0.054409\n",
      "Training: Batch 2/104. Loss of 0.1856, cover loss of 0.0593, secret1 loss of 0.1192, secret2 loss of 0.007025\n",
      "Training: Batch 3/104. Loss of 0.0823, cover loss of 0.0329, secret1 loss of 0.0150, secret2 loss of 0.034405\n",
      "Training: Batch 4/104. Loss of 0.1196, cover loss of 0.0497, secret1 loss of 0.0215, secret2 loss of 0.048368\n",
      "Training: Batch 5/104. Loss of 0.1624, cover loss of 0.0461, secret1 loss of 0.0883, secret2 loss of 0.028049\n",
      "Training: Batch 6/104. Loss of 0.1123, cover loss of 0.0545, secret1 loss of 0.0458, secret2 loss of 0.012062\n",
      "Training: Batch 7/104. Loss of 0.2108, cover loss of 0.1184, secret1 loss of 0.0545, secret2 loss of 0.037911\n",
      "Training: Batch 8/104. Loss of 0.1082, cover loss of 0.0296, secret1 loss of 0.0394, secret2 loss of 0.039191\n",
      "Training: Batch 9/104. Loss of 0.0803, cover loss of 0.0279, secret1 loss of 0.0247, secret2 loss of 0.027786\n",
      "Training: Batch 10/104. Loss of 0.1109, cover loss of 0.0442, secret1 loss of 0.0290, secret2 loss of 0.037786\n",
      "Training: Batch 11/104. Loss of 0.0387, cover loss of 0.0223, secret1 loss of 0.0088, secret2 loss of 0.007525\n",
      "Training: Batch 12/104. Loss of 0.0583, cover loss of 0.0418, secret1 loss of 0.0074, secret2 loss of 0.009128\n",
      "Training: Batch 13/104. Loss of 0.0645, cover loss of 0.0227, secret1 loss of 0.0198, secret2 loss of 0.022082\n",
      "Training: Batch 14/104. Loss of 0.0881, cover loss of 0.0508, secret1 loss of 0.0278, secret2 loss of 0.009485\n",
      "Training: Batch 15/104. Loss of 0.1203, cover loss of 0.0512, secret1 loss of 0.0410, secret2 loss of 0.028080\n",
      "Training: Batch 16/104. Loss of 0.0992, cover loss of 0.0650, secret1 loss of 0.0125, secret2 loss of 0.021734\n",
      "Training: Batch 17/104. Loss of 0.0450, cover loss of 0.0151, secret1 loss of 0.0089, secret2 loss of 0.021015\n",
      "Training: Batch 18/104. Loss of 0.0539, cover loss of 0.0160, secret1 loss of 0.0123, secret2 loss of 0.025626\n",
      "Training: Batch 19/104. Loss of 0.0521, cover loss of 0.0224, secret1 loss of 0.0138, secret2 loss of 0.015895\n",
      "Training: Batch 20/104. Loss of 0.0517, cover loss of 0.0342, secret1 loss of 0.0071, secret2 loss of 0.010421\n",
      "Training: Batch 21/104. Loss of 0.1004, cover loss of 0.0371, secret1 loss of 0.0206, secret2 loss of 0.042704\n",
      "Training: Batch 22/104. Loss of 0.1123, cover loss of 0.0387, secret1 loss of 0.0335, secret2 loss of 0.040080\n",
      "Training: Batch 23/104. Loss of 0.1342, cover loss of 0.0993, secret1 loss of 0.0181, secret2 loss of 0.016792\n",
      "Training: Batch 24/104. Loss of 0.0558, cover loss of 0.0240, secret1 loss of 0.0233, secret2 loss of 0.008498\n",
      "Training: Batch 25/104. Loss of 0.0500, cover loss of 0.0245, secret1 loss of 0.0124, secret2 loss of 0.013083\n",
      "Training: Batch 26/104. Loss of 0.0592, cover loss of 0.0302, secret1 loss of 0.0148, secret2 loss of 0.014262\n",
      "Training: Batch 27/104. Loss of 0.0781, cover loss of 0.0368, secret1 loss of 0.0232, secret2 loss of 0.018106\n",
      "Training: Batch 28/104. Loss of 0.0826, cover loss of 0.0253, secret1 loss of 0.0297, secret2 loss of 0.027554\n",
      "Training: Batch 29/104. Loss of 0.0906, cover loss of 0.0397, secret1 loss of 0.0376, secret2 loss of 0.013273\n",
      "Training: Batch 30/104. Loss of 0.0934, cover loss of 0.0549, secret1 loss of 0.0230, secret2 loss of 0.015497\n",
      "Training: Batch 31/104. Loss of 0.0588, cover loss of 0.0197, secret1 loss of 0.0338, secret2 loss of 0.005336\n",
      "Training: Batch 32/104. Loss of 0.0641, cover loss of 0.0280, secret1 loss of 0.0217, secret2 loss of 0.014408\n",
      "Training: Batch 33/104. Loss of 0.0764, cover loss of 0.0356, secret1 loss of 0.0289, secret2 loss of 0.011888\n",
      "Training: Batch 34/104. Loss of 0.0810, cover loss of 0.0446, secret1 loss of 0.0192, secret2 loss of 0.017187\n",
      "Training: Batch 35/104. Loss of 0.0877, cover loss of 0.0738, secret1 loss of 0.0084, secret2 loss of 0.005578\n",
      "Training: Batch 36/104. Loss of 0.0911, cover loss of 0.0364, secret1 loss of 0.0206, secret2 loss of 0.034096\n",
      "Training: Batch 37/104. Loss of 0.0488, cover loss of 0.0306, secret1 loss of 0.0079, secret2 loss of 0.010360\n",
      "Training: Batch 38/104. Loss of 0.0390, cover loss of 0.0232, secret1 loss of 0.0087, secret2 loss of 0.007042\n",
      "Training: Batch 39/104. Loss of 0.0473, cover loss of 0.0254, secret1 loss of 0.0158, secret2 loss of 0.006047\n",
      "Training: Batch 40/104. Loss of 0.0386, cover loss of 0.0205, secret1 loss of 0.0110, secret2 loss of 0.007048\n",
      "Training: Batch 41/104. Loss of 0.0624, cover loss of 0.0246, secret1 loss of 0.0175, secret2 loss of 0.020360\n",
      "Training: Batch 42/104. Loss of 0.0280, cover loss of 0.0179, secret1 loss of 0.0074, secret2 loss of 0.002784\n",
      "Training: Batch 43/104. Loss of 0.0386, cover loss of 0.0214, secret1 loss of 0.0102, secret2 loss of 0.007021\n",
      "Training: Batch 44/104. Loss of 0.0357, cover loss of 0.0211, secret1 loss of 0.0098, secret2 loss of 0.004682\n",
      "Training: Batch 45/104. Loss of 0.0231, cover loss of 0.0122, secret1 loss of 0.0063, secret2 loss of 0.004566\n",
      "Training: Batch 46/104. Loss of 0.0310, cover loss of 0.0152, secret1 loss of 0.0120, secret2 loss of 0.003855\n",
      "Training: Batch 47/104. Loss of 0.0894, cover loss of 0.0680, secret1 loss of 0.0134, secret2 loss of 0.008001\n",
      "Training: Batch 48/104. Loss of 0.0292, cover loss of 0.0197, secret1 loss of 0.0051, secret2 loss of 0.004486\n",
      "Training: Batch 49/104. Loss of 0.0323, cover loss of 0.0172, secret1 loss of 0.0114, secret2 loss of 0.003683\n",
      "Training: Batch 50/104. Loss of 0.0381, cover loss of 0.0243, secret1 loss of 0.0095, secret2 loss of 0.004302\n",
      "Training: Batch 51/104. Loss of 0.0693, cover loss of 0.0325, secret1 loss of 0.0295, secret2 loss of 0.007331\n",
      "Training: Batch 52/104. Loss of 0.0519, cover loss of 0.0177, secret1 loss of 0.0168, secret2 loss of 0.017415\n",
      "Training: Batch 53/104. Loss of 0.0299, cover loss of 0.0170, secret1 loss of 0.0089, secret2 loss of 0.004034\n",
      "Training: Batch 54/104. Loss of 0.0281, cover loss of 0.0154, secret1 loss of 0.0085, secret2 loss of 0.004140\n",
      "Training: Batch 55/104. Loss of 0.1125, cover loss of 0.0742, secret1 loss of 0.0273, secret2 loss of 0.010998\n",
      "Training: Batch 56/104. Loss of 0.0882, cover loss of 0.0515, secret1 loss of 0.0304, secret2 loss of 0.006376\n",
      "Training: Batch 57/104. Loss of 0.0381, cover loss of 0.0201, secret1 loss of 0.0104, secret2 loss of 0.007522\n",
      "Training: Batch 58/104. Loss of 0.0282, cover loss of 0.0167, secret1 loss of 0.0079, secret2 loss of 0.003614\n",
      "Training: Batch 59/104. Loss of 0.0696, cover loss of 0.0430, secret1 loss of 0.0212, secret2 loss of 0.005481\n",
      "Training: Batch 60/104. Loss of 0.0405, cover loss of 0.0230, secret1 loss of 0.0140, secret2 loss of 0.003391\n",
      "Training: Batch 61/104. Loss of 0.1277, cover loss of 0.0914, secret1 loss of 0.0316, secret2 loss of 0.004697\n",
      "Training: Batch 62/104. Loss of 0.0796, cover loss of 0.0516, secret1 loss of 0.0203, secret2 loss of 0.007806\n",
      "Training: Batch 63/104. Loss of 0.1291, cover loss of 0.0892, secret1 loss of 0.0357, secret2 loss of 0.004214\n",
      "Training: Batch 64/104. Loss of 0.0431, cover loss of 0.0308, secret1 loss of 0.0087, secret2 loss of 0.003516\n",
      "Training: Batch 65/104. Loss of 0.0267, cover loss of 0.0127, secret1 loss of 0.0091, secret2 loss of 0.004829\n",
      "Training: Batch 66/104. Loss of 0.0353, cover loss of 0.0192, secret1 loss of 0.0115, secret2 loss of 0.004630\n",
      "Training: Batch 67/104. Loss of 0.0573, cover loss of 0.0317, secret1 loss of 0.0183, secret2 loss of 0.007333\n",
      "Training: Batch 68/104. Loss of 0.0299, cover loss of 0.0183, secret1 loss of 0.0082, secret2 loss of 0.003382\n",
      "Training: Batch 69/104. Loss of 0.0685, cover loss of 0.0265, secret1 loss of 0.0338, secret2 loss of 0.008150\n",
      "Epoch [1/3], Average_loss: 0.0732\n",
      "Training: Batch 1/104. Loss of 0.0800, cover loss of 0.0300, secret1 loss of 0.0324, secret2 loss of 0.017691\n",
      "Training: Batch 2/104. Loss of 0.0250, cover loss of 0.0149, secret1 loss of 0.0075, secret2 loss of 0.002678\n",
      "Training: Batch 3/104. Loss of 0.0347, cover loss of 0.0251, secret1 loss of 0.0064, secret2 loss of 0.003141\n",
      "Training: Batch 4/104. Loss of 0.0563, cover loss of 0.0357, secret1 loss of 0.0168, secret2 loss of 0.003916\n",
      "Training: Batch 5/104. Loss of 0.0974, cover loss of 0.0579, secret1 loss of 0.0361, secret2 loss of 0.003383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 6/104. Loss of 0.0782, cover loss of 0.0593, secret1 loss of 0.0145, secret2 loss of 0.004470\n",
      "Training: Batch 7/104. Loss of 0.1030, cover loss of 0.0727, secret1 loss of 0.0192, secret2 loss of 0.011081\n",
      "Training: Batch 8/104. Loss of 0.0319, cover loss of 0.0169, secret1 loss of 0.0118, secret2 loss of 0.003173\n",
      "Training: Batch 9/104. Loss of 0.0338, cover loss of 0.0199, secret1 loss of 0.0080, secret2 loss of 0.005985\n",
      "Training: Batch 10/104. Loss of 0.0511, cover loss of 0.0318, secret1 loss of 0.0085, secret2 loss of 0.010767\n",
      "Training: Batch 11/104. Loss of 0.0262, cover loss of 0.0160, secret1 loss of 0.0068, secret2 loss of 0.003447\n",
      "Training: Batch 12/104. Loss of 0.0467, cover loss of 0.0339, secret1 loss of 0.0086, secret2 loss of 0.004179\n",
      "Training: Batch 13/104. Loss of 0.0390, cover loss of 0.0186, secret1 loss of 0.0120, secret2 loss of 0.008414\n",
      "Training: Batch 14/104. Loss of 0.0537, cover loss of 0.0259, secret1 loss of 0.0248, secret2 loss of 0.002964\n",
      "Training: Batch 15/104. Loss of 0.0651, cover loss of 0.0305, secret1 loss of 0.0293, secret2 loss of 0.005324\n",
      "Training: Batch 16/104. Loss of 0.0741, cover loss of 0.0510, secret1 loss of 0.0195, secret2 loss of 0.003554\n",
      "Training: Batch 17/104. Loss of 0.0268, cover loss of 0.0126, secret1 loss of 0.0099, secret2 loss of 0.004330\n",
      "Training: Batch 18/104. Loss of 0.0225, cover loss of 0.0129, secret1 loss of 0.0070, secret2 loss of 0.002564\n",
      "Training: Batch 19/104. Loss of 0.0332, cover loss of 0.0162, secret1 loss of 0.0119, secret2 loss of 0.005082\n",
      "Training: Batch 20/104. Loss of 0.0435, cover loss of 0.0258, secret1 loss of 0.0134, secret2 loss of 0.004341\n",
      "Training: Batch 21/104. Loss of 0.0553, cover loss of 0.0307, secret1 loss of 0.0125, secret2 loss of 0.012056\n",
      "Training: Batch 22/104. Loss of 0.0701, cover loss of 0.0333, secret1 loss of 0.0197, secret2 loss of 0.017158\n",
      "Training: Batch 23/104. Loss of 0.1046, cover loss of 0.0599, secret1 loss of 0.0330, secret2 loss of 0.011713\n",
      "Training: Batch 24/104. Loss of 0.0274, cover loss of 0.0136, secret1 loss of 0.0097, secret2 loss of 0.004117\n",
      "Training: Batch 25/104. Loss of 0.0289, cover loss of 0.0132, secret1 loss of 0.0064, secret2 loss of 0.009255\n",
      "Training: Batch 26/104. Loss of 0.0268, cover loss of 0.0150, secret1 loss of 0.0069, secret2 loss of 0.004937\n",
      "Training: Batch 27/104. Loss of 0.0358, cover loss of 0.0194, secret1 loss of 0.0060, secret2 loss of 0.010443\n",
      "Training: Batch 28/104. Loss of 0.0607, cover loss of 0.0311, secret1 loss of 0.0168, secret2 loss of 0.012772\n",
      "Training: Batch 29/104. Loss of 0.0705, cover loss of 0.0415, secret1 loss of 0.0244, secret2 loss of 0.004575\n",
      "Training: Batch 30/104. Loss of 0.0776, cover loss of 0.0489, secret1 loss of 0.0209, secret2 loss of 0.007786\n",
      "Training: Batch 31/104. Loss of 0.0283, cover loss of 0.0099, secret1 loss of 0.0045, secret2 loss of 0.013866\n",
      "Training: Batch 32/104. Loss of 0.0467, cover loss of 0.0222, secret1 loss of 0.0191, secret2 loss of 0.005371\n",
      "Training: Batch 33/104. Loss of 0.0565, cover loss of 0.0289, secret1 loss of 0.0210, secret2 loss of 0.006574\n",
      "Training: Batch 34/104. Loss of 0.0579, cover loss of 0.0257, secret1 loss of 0.0209, secret2 loss of 0.011271\n",
      "Training: Batch 35/104. Loss of 0.0643, cover loss of 0.0413, secret1 loss of 0.0183, secret2 loss of 0.004841\n",
      "Training: Batch 36/104. Loss of 0.0522, cover loss of 0.0251, secret1 loss of 0.0126, secret2 loss of 0.014476\n",
      "Training: Batch 37/104. Loss of 0.0402, cover loss of 0.0224, secret1 loss of 0.0099, secret2 loss of 0.007930\n",
      "Training: Batch 38/104. Loss of 0.0378, cover loss of 0.0193, secret1 loss of 0.0135, secret2 loss of 0.005031\n",
      "Training: Batch 39/104. Loss of 0.0367, cover loss of 0.0230, secret1 loss of 0.0118, secret2 loss of 0.001860\n",
      "Training: Batch 40/104. Loss of 0.0332, cover loss of 0.0206, secret1 loss of 0.0078, secret2 loss of 0.004865\n",
      "Training: Batch 41/104. Loss of 0.0515, cover loss of 0.0231, secret1 loss of 0.0116, secret2 loss of 0.016784\n",
      "Training: Batch 42/104. Loss of 0.0264, cover loss of 0.0145, secret1 loss of 0.0083, secret2 loss of 0.003518\n",
      "Training: Batch 43/104. Loss of 0.0389, cover loss of 0.0223, secret1 loss of 0.0107, secret2 loss of 0.005836\n",
      "Training: Batch 44/104. Loss of 0.0347, cover loss of 0.0194, secret1 loss of 0.0135, secret2 loss of 0.001811\n",
      "Training: Batch 45/104. Loss of 0.0231, cover loss of 0.0107, secret1 loss of 0.0098, secret2 loss of 0.002637\n",
      "Training: Batch 46/104. Loss of 0.0260, cover loss of 0.0125, secret1 loss of 0.0109, secret2 loss of 0.002575\n",
      "Training: Batch 47/104. Loss of 0.0790, cover loss of 0.0550, secret1 loss of 0.0204, secret2 loss of 0.003685\n",
      "Training: Batch 48/104. Loss of 0.0284, cover loss of 0.0151, secret1 loss of 0.0067, secret2 loss of 0.006557\n",
      "Training: Batch 49/104. Loss of 0.0324, cover loss of 0.0153, secret1 loss of 0.0123, secret2 loss of 0.004820\n",
      "Training: Batch 50/104. Loss of 0.0371, cover loss of 0.0219, secret1 loss of 0.0115, secret2 loss of 0.003645\n",
      "Training: Batch 51/104. Loss of 0.0712, cover loss of 0.0324, secret1 loss of 0.0311, secret2 loss of 0.007644\n",
      "Training: Batch 52/104. Loss of 0.0375, cover loss of 0.0164, secret1 loss of 0.0088, secret2 loss of 0.012257\n",
      "Training: Batch 53/104. Loss of 0.0241, cover loss of 0.0128, secret1 loss of 0.0090, secret2 loss of 0.002333\n",
      "Training: Batch 54/104. Loss of 0.0263, cover loss of 0.0135, secret1 loss of 0.0098, secret2 loss of 0.002946\n",
      "Training: Batch 55/104. Loss of 0.0951, cover loss of 0.0630, secret1 loss of 0.0267, secret2 loss of 0.005460\n",
      "Training: Batch 56/104. Loss of 0.0765, cover loss of 0.0464, secret1 loss of 0.0257, secret2 loss of 0.004409\n",
      "Training: Batch 57/104. Loss of 0.0313, cover loss of 0.0171, secret1 loss of 0.0074, secret2 loss of 0.006821\n",
      "Training: Batch 58/104. Loss of 0.0227, cover loss of 0.0135, secret1 loss of 0.0063, secret2 loss of 0.002945\n",
      "Training: Batch 59/104. Loss of 0.0626, cover loss of 0.0458, secret1 loss of 0.0138, secret2 loss of 0.003091\n",
      "Training: Batch 60/104. Loss of 0.0404, cover loss of 0.0268, secret1 loss of 0.0105, secret2 loss of 0.003094\n",
      "Training: Batch 61/104. Loss of 0.1136, cover loss of 0.0802, secret1 loss of 0.0297, secret2 loss of 0.003698\n",
      "Training: Batch 62/104. Loss of 0.0711, cover loss of 0.0522, secret1 loss of 0.0129, secret2 loss of 0.006051\n",
      "Training: Batch 63/104. Loss of 0.1299, cover loss of 0.0898, secret1 loss of 0.0359, secret2 loss of 0.004223\n",
      "Training: Batch 64/104. Loss of 0.0399, cover loss of 0.0273, secret1 loss of 0.0097, secret2 loss of 0.002899\n",
      "Training: Batch 65/104. Loss of 0.0265, cover loss of 0.0146, secret1 loss of 0.0083, secret2 loss of 0.003661\n",
      "Training: Batch 66/104. Loss of 0.0276, cover loss of 0.0138, secret1 loss of 0.0095, secret2 loss of 0.004297\n",
      "Training: Batch 67/104. Loss of 0.0465, cover loss of 0.0256, secret1 loss of 0.0161, secret2 loss of 0.004854\n",
      "Training: Batch 68/104. Loss of 0.0271, cover loss of 0.0158, secret1 loss of 0.0095, secret2 loss of 0.001693\n",
      "Training: Batch 69/104. Loss of 0.0601, cover loss of 0.0310, secret1 loss of 0.0260, secret2 loss of 0.003097\n",
      "Epoch [2/3], Average_loss: 0.0499\n",
      "Training: Batch 1/104. Loss of 0.0635, cover loss of 0.0331, secret1 loss of 0.0236, secret2 loss of 0.006876\n",
      "Training: Batch 2/104. Loss of 0.0234, cover loss of 0.0133, secret1 loss of 0.0064, secret2 loss of 0.003719\n",
      "Training: Batch 3/104. Loss of 0.0316, cover loss of 0.0214, secret1 loss of 0.0067, secret2 loss of 0.003462\n",
      "Training: Batch 4/104. Loss of 0.0531, cover loss of 0.0338, secret1 loss of 0.0162, secret2 loss of 0.003109\n",
      "Training: Batch 5/104. Loss of 0.0966, cover loss of 0.0507, secret1 loss of 0.0409, secret2 loss of 0.005038\n",
      "Training: Batch 6/104. Loss of 0.0688, cover loss of 0.0413, secret1 loss of 0.0241, secret2 loss of 0.003408\n",
      "Training: Batch 7/104. Loss of 0.0995, cover loss of 0.0603, secret1 loss of 0.0239, secret2 loss of 0.015287\n",
      "Training: Batch 8/104. Loss of 0.0290, cover loss of 0.0159, secret1 loss of 0.0106, secret2 loss of 0.002564\n",
      "Training: Batch 9/104. Loss of 0.0298, cover loss of 0.0148, secret1 loss of 0.0084, secret2 loss of 0.006493\n",
      "Training: Batch 10/104. Loss of 0.0436, cover loss of 0.0217, secret1 loss of 0.0111, secret2 loss of 0.010837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch 11/104. Loss of 0.0235, cover loss of 0.0131, secret1 loss of 0.0067, secret2 loss of 0.003747\n",
      "Training: Batch 12/104. Loss of 0.0421, cover loss of 0.0271, secret1 loss of 0.0117, secret2 loss of 0.003340\n",
      "Training: Batch 13/104. Loss of 0.0393, cover loss of 0.0175, secret1 loss of 0.0115, secret2 loss of 0.010297\n",
      "Training: Batch 14/104. Loss of 0.0551, cover loss of 0.0261, secret1 loss of 0.0265, secret2 loss of 0.002527\n",
      "Training: Batch 15/104. Loss of 0.0582, cover loss of 0.0254, secret1 loss of 0.0281, secret2 loss of 0.004741\n",
      "Training: Batch 16/104. Loss of 0.0718, cover loss of 0.0414, secret1 loss of 0.0259, secret2 loss of 0.004529\n",
      "Training: Batch 17/104. Loss of 0.0251, cover loss of 0.0116, secret1 loss of 0.0089, secret2 loss of 0.004623\n",
      "Training: Batch 18/104. Loss of 0.0198, cover loss of 0.0099, secret1 loss of 0.0070, secret2 loss of 0.002897\n",
      "Training: Batch 19/104. Loss of 0.0288, cover loss of 0.0137, secret1 loss of 0.0109, secret2 loss of 0.004161\n",
      "Training: Batch 20/104. Loss of 0.0398, cover loss of 0.0217, secret1 loss of 0.0146, secret2 loss of 0.003501\n",
      "Training: Batch 21/104. Loss of 0.0512, cover loss of 0.0266, secret1 loss of 0.0143, secret2 loss of 0.010280\n",
      "Training: Batch 22/104. Loss of 0.0648, cover loss of 0.0312, secret1 loss of 0.0183, secret2 loss of 0.015231\n",
      "Training: Batch 23/104. Loss of 0.0913, cover loss of 0.0532, secret1 loss of 0.0296, secret2 loss of 0.008478\n",
      "Training: Batch 24/104. Loss of 0.0226, cover loss of 0.0113, secret1 loss of 0.0075, secret2 loss of 0.003808\n",
      "Training: Batch 25/104. Loss of 0.0223, cover loss of 0.0111, secret1 loss of 0.0064, secret2 loss of 0.004859\n",
      "Training: Batch 26/104. Loss of 0.0229, cover loss of 0.0135, secret1 loss of 0.0068, secret2 loss of 0.002587\n",
      "Training: Batch 27/104. Loss of 0.0284, cover loss of 0.0172, secret1 loss of 0.0055, secret2 loss of 0.005637\n",
      "Training: Batch 28/104. Loss of 0.0490, cover loss of 0.0244, secret1 loss of 0.0174, secret2 loss of 0.007248\n",
      "Training: Batch 29/104. Loss of 0.0712, cover loss of 0.0431, secret1 loss of 0.0235, secret2 loss of 0.004674\n",
      "Training: Batch 30/104. Loss of 0.0714, cover loss of 0.0414, secret1 loss of 0.0213, secret2 loss of 0.008772\n",
      "Training: Batch 31/104. Loss of 0.0167, cover loss of 0.0089, secret1 loss of 0.0044, secret2 loss of 0.003460\n",
      "Training: Batch 32/104. Loss of 0.0526, cover loss of 0.0256, secret1 loss of 0.0151, secret2 loss of 0.011871\n",
      "Training: Batch 33/104. Loss of 0.0544, cover loss of 0.0326, secret1 loss of 0.0175, secret2 loss of 0.004350\n",
      "Training: Batch 34/104. Loss of 0.0444, cover loss of 0.0250, secret1 loss of 0.0151, secret2 loss of 0.004254\n",
      "Training: Batch 35/104. Loss of 0.0608, cover loss of 0.0404, secret1 loss of 0.0165, secret2 loss of 0.003816\n",
      "Training: Batch 36/104. Loss of 0.0447, cover loss of 0.0194, secret1 loss of 0.0136, secret2 loss of 0.011715\n",
      "Training: Batch 37/104. Loss of 0.0339, cover loss of 0.0182, secret1 loss of 0.0093, secret2 loss of 0.006372\n",
      "Training: Batch 38/104. Loss of 0.0328, cover loss of 0.0198, secret1 loss of 0.0098, secret2 loss of 0.003214\n",
      "Training: Batch 39/104. Loss of 0.0350, cover loss of 0.0182, secret1 loss of 0.0134, secret2 loss of 0.003422\n",
      "Training: Batch 40/104. Loss of 0.0291, cover loss of 0.0141, secret1 loss of 0.0096, secret2 loss of 0.005417\n",
      "Training: Batch 41/104. Loss of 0.0424, cover loss of 0.0147, secret1 loss of 0.0120, secret2 loss of 0.015673\n",
      "Training: Batch 42/104. Loss of 0.0248, cover loss of 0.0122, secret1 loss of 0.0080, secret2 loss of 0.004663\n",
      "Training: Batch 43/104. Loss of 0.0386, cover loss of 0.0200, secret1 loss of 0.0142, secret2 loss of 0.004358\n",
      "Training: Batch 44/104. Loss of 0.0324, cover loss of 0.0178, secret1 loss of 0.0115, secret2 loss of 0.003071\n",
      "Training: Batch 45/104. Loss of 0.0225, cover loss of 0.0106, secret1 loss of 0.0055, secret2 loss of 0.006317\n",
      "Training: Batch 46/104. Loss of 0.0249, cover loss of 0.0103, secret1 loss of 0.0118, secret2 loss of 0.002800\n",
      "Training: Batch 47/104. Loss of 0.0775, cover loss of 0.0417, secret1 loss of 0.0319, secret2 loss of 0.003923\n",
      "Training: Batch 48/104. Loss of 0.0267, cover loss of 0.0131, secret1 loss of 0.0082, secret2 loss of 0.005347\n",
      "Training: Batch 49/104. Loss of 0.0276, cover loss of 0.0132, secret1 loss of 0.0118, secret2 loss of 0.002598\n",
      "Training: Batch 50/104. Loss of 0.0400, cover loss of 0.0180, secret1 loss of 0.0160, secret2 loss of 0.006006\n",
      "Training: Batch 51/104. Loss of 0.0701, cover loss of 0.0304, secret1 loss of 0.0321, secret2 loss of 0.007581\n",
      "Training: Batch 52/104. Loss of 0.0377, cover loss of 0.0119, secret1 loss of 0.0139, secret2 loss of 0.011865\n",
      "Training: Batch 53/104. Loss of 0.0263, cover loss of 0.0105, secret1 loss of 0.0115, secret2 loss of 0.004233\n",
      "Training: Batch 54/104. Loss of 0.0241, cover loss of 0.0127, secret1 loss of 0.0072, secret2 loss of 0.004242\n",
      "Training: Batch 55/104. Loss of 0.0935, cover loss of 0.0565, secret1 loss of 0.0323, secret2 loss of 0.004716\n",
      "Training: Batch 56/104. Loss of 0.0767, cover loss of 0.0459, secret1 loss of 0.0250, secret2 loss of 0.005830\n",
      "Training: Batch 57/104. Loss of 0.0295, cover loss of 0.0164, secret1 loss of 0.0073, secret2 loss of 0.005807\n",
      "Training: Batch 58/104. Loss of 0.0248, cover loss of 0.0133, secret1 loss of 0.0091, secret2 loss of 0.002390\n",
      "Training: Batch 59/104. Loss of 0.0568, cover loss of 0.0397, secret1 loss of 0.0143, secret2 loss of 0.002788\n",
      "Training: Batch 60/104. Loss of 0.0406, cover loss of 0.0283, secret1 loss of 0.0091, secret2 loss of 0.003222\n",
      "Training: Batch 61/104. Loss of 0.1111, cover loss of 0.0746, secret1 loss of 0.0335, secret2 loss of 0.002919\n",
      "Training: Batch 62/104. Loss of 0.0685, cover loss of 0.0475, secret1 loss of 0.0155, secret2 loss of 0.005510\n",
      "Training: Batch 63/104. Loss of 0.1271, cover loss of 0.0821, secret1 loss of 0.0416, secret2 loss of 0.003443\n",
      "Training: Batch 64/104. Loss of 0.0414, cover loss of 0.0256, secret1 loss of 0.0121, secret2 loss of 0.003664\n",
      "Training: Batch 65/104. Loss of 0.0281, cover loss of 0.0155, secret1 loss of 0.0090, secret2 loss of 0.003675\n",
      "Training: Batch 66/104. Loss of 0.0264, cover loss of 0.0128, secret1 loss of 0.0101, secret2 loss of 0.003538\n",
      "Training: Batch 67/104. Loss of 0.0475, cover loss of 0.0229, secret1 loss of 0.0197, secret2 loss of 0.004899\n",
      "Training: Batch 68/104. Loss of 0.0288, cover loss of 0.0147, secret1 loss of 0.0125, secret2 loss of 0.001620\n",
      "Training: Batch 69/104. Loss of 0.0626, cover loss of 0.0310, secret1 loss of 0.0286, secret2 loss of 0.003001\n",
      "Epoch [3/3], Average_loss: 0.0467\n"
     ]
    }
   ],
   "source": [
    "net, mean_train_loss, loss_history = train_model_optional(train_set, train_set_secret, train_set_secret_2, train_set_cover, beta, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDnngHn2hNmQ"
   },
   "source": [
    "## Data set \n",
    "\n",
    "https://www.kaggle.com/gaz3ll3/optimization-ii-project-3\n",
    "\n",
    "In order to efficiency, we only consider small pictures, 256x256. If you have problem dealing with 256x256, you can resize them to 128x128 or 64x64.  If you feel the images are too many, you can sample a portion from them as well.  \n",
    "\n",
    "Training and Validation sets are chosen at random (say, 80% and 20%). Each input data will be two images from the training set. \n",
    "\n",
    "If you are more comfortable with other data sets, it is up to you. Say you can use https://tiny-imagenet.herokuapp.com/ for 64x64 small images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw7LJxOGls5E"
   },
   "source": [
    "## Metric\n",
    "\n",
    "In your trainging process for $D$ and $E$, the norm to compare images is the RMSE (root mean squared error), the images are of dimension $N\\times N\\times 3$, BTW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK13JIAnm8dg"
   },
   "source": [
    "## Your final result\n",
    "\n",
    "\n",
    "1.   A writeup on your work, including performance, your work, issues, how do you solve the issues, etc. \n",
    "2.   Test your codes (LSB and NN) against the data set http://r0k.us/graphics/kodak/, each image will be downsize to 256x256 or 128x128 or 64x64 if you trained an NN on smaller images. Report your result in your writeup. \n",
    "3. Code, again, host on github. Submission will be a link. \n",
    "4. If you also tried the optional task, please also report that in your writeup.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_images():\n",
    "    data_lst = []\n",
    "    name_lst = []\n",
    "    \n",
    "    file_dirlist = os.listdir('TestDataSet')\n",
    "    for file_dir in file_dirlist:\n",
    "        split = os.path.splitext(file_dir)\n",
    "        if split[1] == '.png':\n",
    "            file_full_path = os.path.join('TestDataSet', file_dir)\n",
    "            image_tensor = torchvision.io.read_image(file_full_path)\n",
    "            data_lst.append(image_tensor.div(256))\n",
    "            name_lst.append(file_dir)\n",
    "    return data_lst, name_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set, test_data_name = load_test_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3711, 0.3867, 0.3867],\n",
       "         ...,\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.2969, 0.3906, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3945, 0.3867, 0.3867],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3711, 0.3867, 0.3867],\n",
       "         ...,\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.2969, 0.3906, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3945, 0.3867, 0.3867],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3867, 0.3867, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3711, 0.3867, 0.3867],\n",
       "         ...,\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.2969, 0.3906, 0.3867],\n",
       "         [0.3867, 0.3867, 0.3867,  ..., 0.3945, 0.3867, 0.3867],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-99ae47cfea6a>:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_secret = Variable(test_secret, volatile=True)\n",
      "<ipython-input-28-99ae47cfea6a>:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_cover = Variable(test_cover, volatile=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-99ae47cfea6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Compute output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtest_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_secret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_cover\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ana\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6c41487251f4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, secret, secret_2, cover)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mx_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mx_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecret_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mmid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcover\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mx_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_3_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mmid2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "print('test_set')\n",
    "net.eval()\n",
    "net.to('cpu')\n",
    "test_losses = []\n",
    "i = 0\n",
    "while i < len(test_data_set):\n",
    "    test_secret = test_data_set[i+1].float().unsqueeze(1)\n",
    "    test_cover = test_data_set[i].float().unsqueeze(1)\n",
    "    test_cover_name = test_data_name[i]\n",
    "    test_secret_name = test_data_name[i+1]\n",
    "\n",
    "    # Creates variable from secret and cover images\n",
    "    test_secret = Variable(test_secret, volatile=True)\n",
    "    test_cover = Variable(test_cover, volatile=True)\n",
    "    \n",
    "    # Compute output\n",
    "    test_hidden, test_output = net(test_secret, test_cover, 'cpu')\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
    "\n",
    "    print('Test photo cover:', test_cover_name)\n",
    "    print('Test photo secrete:', test_secret_name)\n",
    "    print ('loss: {:.10f} \\nLoss on secret: {:.10f} \\nLoss on cover: {:.10f}\\n'.format(test_loss.data, loss_secret.data, loss_cover.data))\n",
    "        \n",
    "    test_losses.append(test_loss.data)\n",
    "    i += 2\n",
    "        \n",
    "mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "print ('\\nAverage Test loss on test set: {:.10f}'.format(mean_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "“Steg.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
